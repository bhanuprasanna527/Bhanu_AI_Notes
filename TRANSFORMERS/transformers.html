<!DOCTYPE html>
<html lang="en"> 
<head>
    <title>Transformers | Bhanu.AI Notes</title>
    
    <!-- Meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="AI Notes written by Bhanu Prasanna from Various Specializations.">
    <meta name="author" content="Bhanu prasanna">    
    <link rel="shortcut icon" href="/favicon.ico">
    
    <!-- Google Font -->
    <link href="https://fonts.googleapis.com/css?family=Poppins:300,400,500,600,700&display=swap" rel="stylesheet">
    
    <!-- FontAwesome JS-->
    <script defer src="/assets/fontawesome/js/all.min.js"></script>
    
    <!-- Plugins CSS -->
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.2/styles/atom-one-dark.min.css">
    <link rel="stylesheet" href="/assets/plugins/simplelightbox/simple-lightbox.min.css">

    <!-- Theme CSS -->  
    <link id="theme-style" rel="stylesheet" href="/assets/css/theme.css">

</head> 

<body class="docs-page">    
    <header class="fixed-top">	    
        <div class="branding docs-branding">
            <div class="container-fluid position-relative py-2">
                <div class="docs-logo-wrapper">
					<button id="docs-sidebar-toggler" class="docs-sidebar-toggler docs-sidebar-visible me-2 d-xl-none" type="button">
	                    <span></span>
	                    <span></span>
	                    <span></span>
	                </button>
	                <div class="site-logo"><a class="navbar-brand" href="/index.html"><img class="logo-icon me-2" src="/assets/images/coderdocs-logo.svg" alt="logo"><span class="logo-text">Bhanu<span class="text-alt">.AI</span></span></a></div>    
                </div><!--//docs-logo-wrapper-->
	            <div class="docs-top-utilities d-flex justify-content-end align-items-center">
	                <div class="top-search-box d-none d-lg-flex">
		                <form class="search-form">
				            <input type="text" placeholder="Search the docs..." name="search" class="form-control search-input">
				            <button type="submit" class="btn search-btn" value="Search"><i class="fas fa-search"></i></button>
				        </form>
	                </div>
	
					<ul class="social-list list-inline mx-md-3 mx-lg-5 mb-0 d-none d-lg-flex">
						<li class="list-inline-item"><a href="https://github.com/bhanuprasanna527" target="_blank" rel="noopener noreferrer"><i class="fab fa-github fa-fw"></i></a></li>
			            <li class="list-inline-item"><a href="https://twitter.com/bhanu527" target="_blank" rel="noopener noreferrer"><i class="fab fa-twitter fa-fw"></i></a></li>
		                <li class="list-inline-item"><a href="https://www.linkedin.com/in/bhanu-prasanna-a1b71b1a2/" target="_blank" rel="noopener noreferrer"><i class="fab fa-linkedin fa-fw"></i></a></li>
		                <li class="list-inline-item"><a href="https://www.instagram.com/bhanuprasanna527/" target="_blank" rel="noopener noreferrer"><i class="fab fa-instagram fa-fw"></i></a></li>
		            </ul><!--//social-list-->
		            <a href="https://github.com/bhanuprasanna527/Bhanu_AI_Notes" class="btn btn-primary d-none d-lg-flex" target="_blank" rel="noopener noreferrer">GITHUB 🌟</a>
	            </div><!--//docs-top-utilities-->
            </div><!--//container-->
        </div><!--//branding-->
    </header><!--//header-->
    
    
    <div class="docs-wrapper">
	    <div id="docs-sidebar" class="docs-sidebar">
		    <div class="top-search-box d-lg-none p-3">
                <form class="search-form">
		            <input type="text" placeholder="Search the docs..." name="search" class="form-control search-input">
		            <button type="submit" class="btn search-btn" value="Search"><i class="fas fa-search"></i></button>
		        </form>
            </div>
		    <nav id="docs-nav" class="docs-nav navbar">
			    <ul class="section-items list-unstyled nav flex-column pb-3">
				    <li class="nav-item section-title"><a class="nav-link scrollto active" href="#section-1"><span class="theme-icon-holder me-2"><i class="fa-solid fa-1"></i></span>Transformers</a></li>
				    <li class="nav-item"><a class="nav-link scrollto" href="#item-1-1">Introduction</a></li>
				    <li class="nav-item"><a class="nav-link scrollto" href="#item-1-2">Transformers vs RNN</a></li>
				    <li class="nav-item"><a class="nav-link scrollto" href="#item-1-3">Transformer Architecture</a></li>
				    <li class="nav-item section-title mt-3"><a class="nav-link scrollto" href="#section-2"><span class="theme-icon-holder me-2"><i class="fa-solid fa-2"></i></span>Encoder Stack</a></li>
				    <li class="nav-item"><a class="nav-link scrollto" href="#item-2-1">Multi-Head Attention</a></li>
				    <li class="nav-item"><a class="nav-link scrollto" href="#item-2-2">Scaled Dot Product Attention</a></li>
				    <li class="nav-item"><a class="nav-link scrollto" href="#item-2-3">Positional Encoding</a></li>
					<li class="nav-item"><a class="nav-link scrollto" href="#item-2-4">Position-wise Feed-Forward Networks</a></li>
				    <li class="nav-item section-title mt-3"><a class="nav-link scrollto" href="#section-3"><span class="theme-icon-holder me-2"><i class="fa-solid fa-3"></i></span>Decoder Stack</a></li>
				    <li class="nav-item"><a class="nav-link scrollto" href="#item-3-1">Masked Multi Head Attention</a></li>
				    <li class="nav-item"><a class="nav-link scrollto" href="#item-3-2">Transformer Encoder Again</a></li>
				    <li class="nav-item"><a class="nav-link scrollto" href="#item-3-3">Linear and Softmax</a></li>
				    <li class="nav-item section-title mt-3"><a class="nav-link scrollto" href="#section-4"><span class="theme-icon-holder me-2"><i class="fa-solid fa-4"></i></span>Conclusion</a></li>
				    <li class="nav-item"><a class="nav-link scrollto" href="#item-4-1">What we have covered?</a></li>
			    </ul>

		    </nav><!--//docs-nav-->
	    </div><!--//docs-sidebar-->
	    <div class="docs-content">
		    <div class="container">
			    <article class="docs-article" id="section-1">
				    <header class="docs-header">
					    <h1 class="docs-heading">Transformers</h1>
					    <section class="docs-intro">
							<img class="figure-img img-fluid shadow rounded" src="https://www.pvamu.edu/cahs/wp-content/uploads/sites/27/bumblebee-transformers.jpg" alt="" title="Transformers"/></a><br><br>
						    <p>We are not definetly talking about the Transformer. But we are moving towards a path to build a Transformer. Transformer is a new technique that is orginally used for Language translation purpose but it is being used for many applications and mainly used for NLP applications.</p>
				    </header>
				    <section class="docs-section" id="item-1-1">
						<h2 class="section-heading">Introduction</h2>
						<p><b>Let's Look at a Fact:</b></p>
						<p>GPT - 3, and it's previous versions are all built using the Transformers. GPT full form being Generative Pre-Trained Transformer. Many models that are showing massive results are being built with the help of Transformers. Let's Discuss more about the Architecture and Where it all started in upcoming Sections.</p><br>
						<p>Some points about Transformers:</p>
						<li>Transformer is based upon the Attention System which is amazingly described in the Research Article Attention is All you Need.</li>
						<li>Transformer has mainly 2 methods present inside its Architecture which are Encoder and Decoder layers which makes the Transformers a Semi-Supervised Learning Model.</li>
						<LI>Let's look at a simple representation of the Transformer Architecture:</LI>
						<img class="figure-img img-fluid shadow rounded" src="Untitled Diagram.drawio.png" alt="" title="Simple Representation of Transformer Architecture">
						<li>Here we can see that the Encoder takes the Input Sequence and the Decoder will take the Output Sequence which we will unravel each part of the architecture in detail.</li>
					</section><!--//section-->
					
					<section class="docs-section" id="item-1-2">
						<h2 class="section-heading">Transformers vs RNN</h2>
						<li>There is a huge debate within the community that which is best for Sequence Tasks. No doubt RNN is a Sequential Model Leader. But Transformer does not implement in a Sequence. RNN was used previously for NLP or Machine Translation tasks.</li>
						<li>This has now been take over by Transformers for its efficient process in dealing with the NLP tasks.</li>
						<li>The training time for the Transformer was significantly reduced because of the parallelization process which made the Transformer more efficient than RNN.</li>
						<li>RNN processes one input at a time while Transformer take all the Input at Once. RNN retains previous information using LSTM architecture which are the hidden states first developed to solve the Vanishing Gradient problem in RNN.</li>
						<li>Because of being Non-Sequential the Transformer do not suffer from Long Dependency issues.</li>
						<li>Positional Encodings has been developed to replace Recurrence. We will discuss more about Positional Encoding in the upcoming Sections.</li>
					</section><!--//section-->
					
					<section class="docs-section" id="item-1-3">
						<h2 class="section-heading">Transformer Architecture</h2>
						<p>Let us look at the Architecture first before getting breaking it up:</p>
						<img class="figure-img img-fluid shadow rounded" src="https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png" alt="" title="Transformer Architecture" width="400" 
						height="500">
						<br><br>
						<li>The Transformer Architecture consists of two main stacks Encoder and Decoder Stack.</li>
					</section><!--//section-->
					
			    </article>
			    
			    <article class="docs-article" id="section-2">
				    <header class="docs-header">
					    <h1 class="docs-heading">Encoder Stack</h1>
					    <section class="docs-intro">
						    <p>Let us understand more about Encoder Stack in Transformer.</p>
						<img class="figure-img img-fluid shadow rounded" src="https://www.researchgate.net/publication/334288604/figure/fig1/AS:778232232148992@1562556431066/The-Transformer-encoder-structure.ppm" alt="" title="Encoder Stack"><br><br>
						<li>Encoder Stack consists of mainly 2 parts:</li>
						<ol>
							<li>Multi-Head Attention</li>
							<li>Feed Forward Neural Network</li>
						</ol>
						</section><!--//docs-intro-->
				    </header>


					<section class="docs-section" id="item-2-1">
						<h2 class="section-heading">Multi-Head Attention</h2>
						<p>Let us look at the architecture of the Multi Head Attention:</p>
						<img class="figure-img img-fluid shadow rounded" src="https://production-media.paperswithcode.com/methods/multi-head-attention_l1A3G7a.png" alt="" title="Multi Head Attention"><br>
						<li>Instead of using a single attention function with d<sub>model</sub> - dimensional keys, values, and queries, it is found beneficial when the queries, keys, and values are linearly projected h times with different, learned projections to d<sub>k</sub>, d<sub>k</sub> and d<sub>v</sub> dimensions.</li>
						<li>Each head (h<sup>th</sup>) value produces a projected version of queries, keys, and values where the attention function is performed in parallel, yielding d<sub>v</sub> - dimensional Output values.</li>
						<li>These are concatenated and once again projected, resulting in the final values depicted in the image below.</li>
						<li>Multiple attention heads allows for attending to parts of the sequence differently (e.g. longer-term dependencies versus shorter-term dependencies).</li>
						<p>$$ \text{MultiHead}\left(\textbf{Q}, \textbf{K}, \textbf{V}\right) = \left[\text{head}_{1},\dots,\text{head}_{h}\right]\textbf{W}_{0}$$</p>
						<p>$$\text{where} \text{ head}_{i} = \text{Attention} \left(\textbf{Q}\textbf{W}_{i}^{Q}, \textbf{K}\textbf{W}_{i}^{K}, \textbf{V}\textbf{W}_{i}^{V} \right) $$</p>
						<li>Above <math xmlns="http://www.w3.org/1998/Math/MathML">
							<mrow data-mjx-texclass="ORD">
							  <mtext mathvariant="bold">W</mtext>
							</mrow>
						  </math> are all learnable parameter matrices.</li>
						  <li>Where the projections are parameter matrices are:</li>
						  <p>$$W_{i}^{Q}\in \mathbb{R}^{d_{model} \times d_{k}}$$</p>
						  <p>$$W_{i}^{K}\in \mathbb{R}^{d_{model} \times d_{k}}$$</p>
						  <p>$$W_{i}^{V}\in \mathbb{R}^{d_{model} \times d_{v}}$$</p>
						  <p>$$W^{O}\in \mathbb{R}^{hd_{v} \times d_{model}}$$</p>

						<br>
					
						
					</section><!--//section-->
					<section class="docs-section" id="item-2-2">
						<h2 class="section-heading">Scaled Dot Product Attention</h2>
						<li>In the image we can see that the Input to the Multi Head Attention are 3 Matrices that are Query (Q), Key (K), Value (V) which are sent into a Linear module which is a Feed Forward Neural Network (used 'linear' activation function). Let us at how the Scaled Dot Product is performed:</li>
						<img class="figure-img img-fluid shadow rounded" src="https://production-media.paperswithcode.com/methods/SCALDE.png" alt="" title="Scaled Dot Product Attention">
						<br>
						<li>It is a Attention Mechanism introduced in the Research paper of Attention is all you need.</li>
						<li>Scaled Dot Product Attention is an Attention Mechanism where the dot product of Query with all Keys , divided using <math xmlns="http://www.w3.org/1998/Math/MathML">
							<msqrt>
							  <msub>
								<mi>d</mi>
								<mi>k</mi>
							  </msub>
							</msqrt>
						  </math>, and apply Softmax Function to obtain the weights on the values.</li>
						  <li>In the Attention Mechanism, the queries are packed together into Q, same with keys and values are packed into K and V. This is where the Query (Q), Key (K), and Value (V) vectors that are passed through a Linear module then we calculate the Attention using the equation:</li>
						  <p>$$ {\text{Attention}}(Q, K, V) = \text{softmax}\left(\frac{QK^{T}}{\sqrt{d_k}}\right)V $$</p>
						  <li>There are two types of General Attention Mechanism which are Additive and Dot Product (multiplicative) attention mechanisms. The Dot Product is similar to the Attention that we are using Scaled Dot Product where we use a Scaling factor which is <math xmlns="http://www.w3.org/1998/Math/MathML">
							<mfrac>
								<mi>1</mi>
							<msqrt>
							  <msub>
								<mi>d</mi>
								<mi>k</mi>
							  </msub>
							</msqrt>
						</mfrac>
						  </math>.</li>
						  <li>Additive Attention computes the Compatability funciton using Feed-Forward Network with a Single Hidden Layer.</li>
						  <li>The Additive and Dot Product Attention are quite similar but dot-product attention ismuch faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.</li>
						  <li><math xmlns="http://www.w3.org/1998/Math/MathML">
						
							  <msub>
								<mi>d</mi>
								<mi>k</mi>
							  </msub>
						
						  </math> is the dimension of the Query and Key vector. For small values of <math xmlns="http://www.w3.org/1998/Math/MathML">
						
							<msub>
							  <mi>d</mi>
							  <mi>k</mi>
							</msub>
					  
						</math> the two attention mechanisms work fine. Additive Attention outperforms Dot Product Attention when <math xmlns="http://www.w3.org/1998/Math/MathML">
						
							<msub>
							  <mi>d</mi>
							  <mi>k</mi>
							</msub>
					  
						</math> is larger. For larger values of <math xmlns="http://www.w3.org/1998/Math/MathML">
						
							<msub>
							  <mi>d</mi>
							  <mi>k</mi>
							</msub>
					  
						</math>, the dot product grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients.</li>
						<li>To counteract this effect, we scale the dot products by <math xmlns="http://www.w3.org/1998/Math/MathML">
							<mfrac>
								<mi>1</mi>
							<msqrt>
							  <msub>
								<mi>d</mi>
								<mi>k</mi>
							  </msub>
							</msqrt>
						</mfrac>
						  </math>.</li>
					</section><!--//section-->

					<section class="docs-section" id="item-2-3">
						<h2 class="section-heading">Positional Encoding</h2>
						<li>Since the Transformer contains no recurrence and no convolution, in order for the model to make use of theorder of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence.</li>
						<li>To this end, we add "positional encodings" to the input embeddings at thebottoms of the encoder and decoder stacks. The positional encodings have the same dimension <math xmlns="http://www.w3.org/1998/Math/MathML">
						
							<msub>
							  <mi>d</mi>
							  <mi>model</mi>
							</msub>
					  
						</math>as the embeddings, so that the two can be summed. There are many choices of positional encodings,learned and fixed.</li>
						<li>In this work, we use sine and cosine functions of different frequencies:</li>
						<p>$$PE_{(pos, 2i)} = sin(pos/10000^{2i/d_{model}})$$</p>
						<p>$$PE_{(pos, 2i+1)} = cos(pos/10000^{2i/d_{model}})$$</p>
						<li>Where pos is the position and i is the dimension. That is, each dimension of the positional encodingcorresponds to a sinusoid. </li>
						<li>The wavelengths form a geometric progression from 2π to 10000·2π. We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset k, PE<sub>pos+k</sub> can be represented as a linear function of PE<sub>pos</sub>.</li>
						<li>We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.</li>
					</section><!--//section-->
					
					<section class="docs-section" id="item-2-4">
						<h2 class="section-heading">Position-wise Feed-Forward Networks</h2>
						<li>In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between.</li>
						<p>$$FFN(x) = max(0, xW_{1}+b_{1})W_{2}+b_{2}$$</p>
						<li>While the linear transformations are the same across different positions, they use different parametersfrom layer to layer.</li>
						<li>Another way of describing this is as two convolutions with kernel size 1.The dimensionality of input and output is d<sub>model</sub> = 512, and the inner-layer has dimensionality d<sub>ff</sub> = 2048.</li>
					</section><!--//section-->
				     
			    </article><!--//docs-article-->
			    
			    
			    <article class="docs-article" id="section-3">
				    <header class="docs-header">
					    <h1 class="docs-heading">Decoder Stack</h1>
					    <section class="docs-intro">
						    <p>Let us understand more about the Decoder Stack in Transformer:</p>
							<img class="figure-img img-fluid shadow rounded" src="attention_research_1.png" alt="" title="Decoder Stack"width="400" 
							height="500"><br><br>
							<li>The layers in the decoder stack are:</li>
							<ol>
								<li>Multi Head Attention with Mask</li>
								<li>Transformer Encoder Layer</li>
								<li>Linear and Softmax</li>
							</ol>
						</section><!--//docs-intro-->
				    </header>
				     <section class="docs-section" id="item-3-1">
						<h2 class="section-heading">Masked Multi Head Attention</h2>
						<li>In Transformer Encoder Stack we have used normal Multi Head Attention whereas in Decoder we use Mask.</li>
						<li>Masking is needed to prevent the attention mechanism of a transformer from “cheating” in the decoder when training (on a translating task for instance). This kind of “ cheating-proof masking” is not present in the encoder side.</li>
						<li>This is the purpose of Masking in the Multi Head Attention in the Decoder Stack.</li>
						<li>An article explaining the importance of Masking in Transformer <b><a href="https://medium.com/analytics-vidhya/masking-in-transformers-self-attention-mechanism-bad3c9ec235c">LINK</a></b>.</li>
					</section><!--//section-->
					
					<section class="docs-section" id="item-3-2">
						<h2 class="section-heading">Transformer Encoder Again</h2>
						<p>Here the Transformer Encoder layer is once again used. The whole explanation is provided above for each and every step.</p>
					</section><!--//section-->
					
				
					<section class="docs-section" id="item-3-3">
						<h2 class="section-heading">Linear and Softmax</h2>
						<p>At last we enter the Linear which is a Feed Forward Neural Network with Linear activation function.</p>
						<h3>Embeddings and Softmax</h3>
						<li>Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension d<sub>model</sub>.</li>
						<li>We also use the usual learned linear transfor-mation and softmax function to convert the decoder output to predicted next-token probabilities.</li>
						<li>In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation. In the embedding layers, we multiply those weights by <math xmlns="http://www.w3.org/1998/Math/MathML">
							
							<msqrt>
							  <msub>
								<mi>d</mi>
								<mi>model</mi>
							  </msub>
							</msqrt>
						
						  </math>.</li>
					</section><!--//section-->

					<article class="docs-article" id="section-4">
						<header class="docs-header">
							<h1 class="docs-heading">Conclusion</h1>
							<section class="docs-intro">
								<p>We have learnt about the Transformer in Detail.</p>
							</section><!--//docs-intro-->
						</header>
						 <section class="docs-section" id="item-4-1">
							<h2 class="section-heading">What we have covered?</h2>
							<ol>
							<li>Learnt about Transformer Architecture</li>
							<li>Went deep into the Architecture</li>
							<li>Leant about Encoder and Decoder</li>
						</ol>
						<li>The Transformer is developed by Google Deep Mind and the link to the amazing article is <b><a href="https://arxiv.org/abs/1706.03762">LINK</a></b>.</li>
						</section><!--//section-->					
					</article><!--//docs-article-->
			    </article><!--//docs-article-->
			    
				

			    <footer class="footer">
				    <div class="container text-center py-5">
				        <!--/* This template is free as long as you keep the footer attribution link. If you'd like to use the template without the attribution link, you can buy the commercial license via our website: themes.3rdwavemedia.com Thank you for your support. :) */-->
			            <!-- <small class="copyright">Designed with <span class="sr-only">love</span><i class="fas fa-heart" style="color: #fb866a;"></i> by <a class="theme-link" href="http://themes.3rdwavemedia.com" target="_blank">Xiaoying Riley</a> for developers</small> -->
				        <ul class="social-list list-unstyled pt-4 mb-0">
						    <li class="list-inline-item"><a href="https://github.com/bhanuprasanna527" target="_blank" rel="noopener noreferrer"><i class="fab fa-github fa-fw"></i></a></li>
			            <li class="list-inline-item"><a href="https://twitter.com/bhanu527" target="_blank" rel="noopener noreferrer"><i class="fab fa-twitter fa-fw"></i></a></li>
		                <li class="list-inline-item"><a href="https://www.linkedin.com/in/bhanu-prasanna-a1b71b1a2/" target="_blank" rel="noopener noreferrer"><i class="fab fa-linkedin fa-fw"></i></a></li>
		                <li class="list-inline-item"><a href="https://www.instagram.com/bhanuprasanna527/" target="_blank" rel="noopener noreferrer"><i class="fab fa-instagram fa-fw"></i></a></li>
				        </ul><!--//social-list-->
				    </div>
			    </footer>
		    </div> 
	    </div>
    </div><!--//docs-wrapper-->
   
       
    <!-- Javascript -->          
    <script src="/assets/plugins/popper.min.js"></script>
    <script src="/assets/plugins/bootstrap/js/bootstrap.min.js"></script>  

	<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    
    <!-- Page Specific JS -->
    <script src="/assets/plugins/smoothscroll.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.8/highlight.min.js"></script>
    <script src="/assets/js/highlight-custom.js"></script> 
    <script src="/assets/plugins/simplelightbox/simple-lightbox.min.js"></script>      
    <script src="/assets/plugins/gumshoe/gumshoe.polyfills.min.js"></script> 
    <script src="/assets/js/docs.js"></script> 

</body>
</html> 

