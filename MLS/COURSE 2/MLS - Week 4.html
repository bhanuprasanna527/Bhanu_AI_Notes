<!DOCTYPE html>
<html lang="en"> 
<head>
    <title>Bhanu.AI Machine Learning Specialization</title>
    
    <!-- Meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="AI Notes written by Bhanu Prasanna from Various Specializations.">
    <meta name="author" content="Bhanu prasanna">    
    <link rel="shortcut icon" href="/favicon.ico">

	<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    
    <!-- Google Font -->
    <link href="https://fonts.googleapis.com/css?family=Poppins:300,400,500,600,700&display=swap" rel="stylesheet">
    
    <!-- FontAwesome JS-->
    <script defer src="/assets/fontawesome/js/all.min.js"></script>
    
    <!-- Plugins CSS -->
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.2/styles/atom-one-dark.min.css">
    <link rel="stylesheet" href="/assets/plugins/simplelightbox/simple-lightbox.min.css">

    <!-- Theme CSS -->  
    <link id="theme-style" rel="stylesheet" href="/assets/css/theme.css">

</head> 

<body class="docs-page">    
    <header class="fixed-top">	    
        <div class="branding docs-branding">
            <div class="container-fluid position-relative py-2">
                <div class="docs-logo-wrapper">
					<button id="docs-sidebar-toggler" class="docs-sidebar-toggler docs-sidebar-visible me-2 d-xl-none" type="button">
	                    <span></span>
	                    <span></span>
	                    <span></span>
	                </button>
	                <div class="site-logo"><a class="navbar-brand" href="/index.html"><img class="logo-icon me-2" src="/assets/images/coderdocs-logo.svg" alt="logo"><span class="logo-text">Bhanu<span class="text-alt">.AI</span></span></a></div>    
                </div><!--//docs-logo-wrapper-->
	            <div class="docs-top-utilities d-flex justify-content-end align-items-center">
	                <div class="top-search-box d-none d-lg-flex">
		                <form class="search-form">
				            <input type="text" placeholder="Search the docs..." name="search" class="form-control search-input">
				            <button type="submit" class="btn search-btn" value="Search"><i class="fas fa-search"></i></button>
				        </form>
	                </div>
	
					<ul class="social-list list-inline mx-md-3 mx-lg-5 mb-0 d-none d-lg-flex">
						<li class="list-inline-item"><a href="https://github.com/bhanuprasanna527" target="_blank" rel="noopener noreferrer"><i class="fab fa-github fa-fw"></i></a></li>
			            <li class="list-inline-item"><a href="https://twitter.com/bhanu527" target="_blank" rel="noopener noreferrer"><i class="fab fa-twitter fa-fw"></i></a></li>
		                <li class="list-inline-item"><a href="https://www.linkedin.com/in/bhanu-prasanna-a1b71b1a2/" target="_blank" rel="noopener noreferrer"><i class="fab fa-linkedin fa-fw"></i></a></li>
		                <li class="list-inline-item"><a href="https://www.instagram.com/bhanuprasanna527/" target="_blank" rel="noopener noreferrer"><i class="fab fa-instagram fa-fw"></i></a></li>
		            </ul><!--//social-list-->
		            <a href="https://github.com/bhanuprasanna527/Bhanu_AI_Notes" class="btn btn-primary d-none d-lg-flex" target="_blank" rel="noopener noreferrer">GITHUB 🌟</a>
	            </div><!--//docs-top-utilities-->
            </div><!--//container-->
        </div><!--//branding-->
    </header><!--//header-->
    
    
    <div class="docs-wrapper">
	    <div id="docs-sidebar" class="docs-sidebar">
		    <div class="top-search-box d-lg-none p-3">
                <form class="search-form">
		            <input type="text" placeholder="Search the docs..." name="search" class="form-control search-input">
		            <button type="submit" class="btn search-btn" value="Search"><i class="fas fa-search"></i></button>
		        </form>
            </div>
		    <nav id="docs-nav" class="docs-nav navbar">
			    <ul class="section-items list-unstyled nav flex-column pb-3">
				    <li class="nav-item section-title"><a class="nav-link scrollto active" href="#section-1"><span class="theme-icon-holder me-2"><i class="fa-solid fa-1"></i></span>Decision Trees</a></li>
				    <li class="nav-item"><a class="nav-link scrollto" href="#item-1-1">Theory of Decision Tree</a></li>
				    <li class="nav-item"><a class="nav-link scrollto" href="#item-1-2">Decision Tree Learning</a></li>
				    <li class="nav-item section-title mt-3"><a class="nav-link scrollto" href="#section-2"><span class="theme-icon-holder me-2"><i class="fa-solid fa-2"></i></span>Decision Tree Learning</a></li>
				    <li class="nav-item"><a class="nav-link scrollto" href="#item-2-1">Measuring Purity</a></li>
				    <li class="nav-item"><a class="nav-link scrollto" href="#item-2-2">Information Gain</a></li>
				    <li class="nav-item"><a class="nav-link scrollto" href="#item-2-3">Putting it together</a></li>
					<li class="nav-item"><a class="nav-link scrollto" href="#item-2-4">One Hot Encoding</a></li>
					<li class="nav-item"><a class="nav-link scrollto" href="#item-2-5">Continuous Valued Features</a></li>
					<li class="nav-item"><a class="nav-link scrollto" href="#item-2-6">Regression Trees</a></li>
				    <li class="nav-item section-title mt-3"><a class="nav-link scrollto" href="#section-3"><span class="theme-icon-holder me-2"><i class="fa-solid fa-3"></i></span>Tree Ensembles</a></li>
				    <li class="nav-item"><a class="nav-link scrollto" href="#item-3-1">Sampling with Replacement</a></li>
				    <li class="nav-item"><a class="nav-link scrollto" href="#item-3-2">Random Forest Algorithm</a></li>
				    <li class="nav-item"><a class="nav-link scrollto" href="#item-3-3">XGBoost</a></li>
					<li class="nav-item"><a class="nav-link scrollto" href="#item-3-4">When to use Decision Tree?</a></li>
				    <li class="nav-item section-title mt-3"><a class="nav-link scrollto" href="#section-4"><span class="theme-icon-holder me-2"><i class="fa-solid fa-4"></i></span>Conclusion</a></li>
				    <li class="nav-item"><a class="nav-link scrollto" href="#item-4-1">What we have covered?</a></li>
			    </ul>

		    </nav><!--//docs-nav-->
	    </div><!--//docs-sidebar-->
	    <div class="docs-content">
		    <div class="container">
			    <article class="docs-article" id="section-1">
				    <header class="docs-header">
					    <h1 class="docs-heading">Decision Trees<span class="docs-time">Last updated: 2019-06-01</span></h1>
					    <section class="docs-intro">
						    <p>Let us start from first understanding what is a tree and you might have assumed it is also a Data Structure which is a concept of Computer Science.</p>
							<p>A Tree Data Structure is one of algorithms that represents hierarchical data. Some key points about tree data structure:</p>
							<li>Tree data structure is a collection of objects known as nodes that are linked together to simulate heirarchy.</li>
							<li>Tree data structure is a non-linear data structure as the data represented is not present in a sequential manner.</li>
							<li>The top most node is called the root node, and below those are called the internal nodes and the bottomost are called as leaf nodes.</li>
							<img class="figure-img img-fluid shadow rounded" src="https://miro.medium.com/max/1400/0*IS9xKHt83nuERC9P" alt="" title="Tree Data Structure">
							<li>Each node consists some sort of data present and a link to other nodes often called as children nodes.</li>
						</section><!--//docs-intro-->
						
				    </header>
				    <section class="docs-section" id="item-1-1">
						<h2 class="section-heading">Theory of Decision Tree</h2>
						<p>Decision Tree is like a flowchart that represents data in a hierarchical structure. Each node in the structure represents a test node for feature, where the node classifies the provided test.</p>
						<p>Let us take the example of Rain or Sunny, the features are Humidity and Wind. The below image clearly demonstrates the point:</p>
						<img class="figure-img img-fluid shadow rounded" src="https://www.saedsayad.com/images/Decision_Tree_1.png" alt="" title="Rainy or Sunny Forecast">
						<p>This is how the Decision Tree functions. Let us understand more about Decision Tree.</p>
					</section><!--//section-->
					
					<section class="docs-section" id="item-1-2">
						<h2 class="section-heading">Decision Tree Learning</h2>
						<p>There are mainly two decisions that are taken care when developing a Decision Tree model:</p>
						<p><b>Decision 1: </b> How to choose what feature to split on at each node?</p>
						<p>Maximize purity (or minimize impurity)</p>
						<p><b>Purity:</b> The decision to split at each node is made according to the metric called purity. A node is 100% impure when a node is split evenly 50/50 and 100% pure when all of its data belongs to a single class.</p>
						<p>The proportion of data elements in the group that belong to the subset is the basis for the idea of purity in the construction of decision trees.</p>
						<p><b>Decision 2: </b>When do you stop splitting?</p>
						<li>When a node is 100% one class.</li>
						<li>When splitting a node will result in the tree exceeding a maximum depth.</li>
						<li>When improvements in purity score are below a threshold.</li>
						<li>When number of examples in a node is below a threshold.</li>
						<br>
						<p>Let's see through a gif:</p>
						<img class="figure-img img-fluid shadow rounded" src="ezgif.com-gif-maker.gif" alt="" title="Decision Tree Learning">
					</section><!--//section-->
					
			    </article>
			    
			    <article class="docs-article" id="section-2">
				    <header class="docs-header">
					    <h1 class="docs-heading">Decision Tree Learning</h1>
					    <section class="docs-intro">
						    <p>In this section we will learn more about the process of Decision Tree Learning.</p>
						</section><!--//docs-intro-->
				    </header>
				     <section class="docs-section" id="item-2-1">
						<h2 class="section-heading">Measuring Purity</h2>
						<p>Entropy is measured by using the probability of the model that predicted the classes. Let us see through a Diagram:</p>
						<img class="figure-img img-fluid shadow rounded" src="Screenshot 2022-10-05 at 11.53.12 PM.png" alt="" title="Entropy as Measure of Impurity">
						<br><br>
						<p>As you can see in the above diagram that the entropy graph looks like downward convex. Here, when the ratio is 50/50 then the entropy is at the max. Rest of the area in the entropy graph shows a part of how much entropy it produces.</p>
						<P><b>Let us look at the equation for entropy:</b></P>
						<img class="figure-img img-fluid shadow rounded" src="Screenshot 2022-10-05 at 11.47.33 PM.png" alt="" title="Equation and Graph for Entropy">
						<br><br>
						<li><b>There is also another equation that is also used for impurity: Gini index.</b></li>
					</section><!--//section-->
					
					<section class="docs-section" id="item-2-2">
						<h2 class="section-heading">Information Gain</h2>
						<li>The information gain criteria lets you decide how to choose one feature to split a one-node.</li>
						<p>Informatin gain is used for testing the split that is formulated at the node. The node is feature like ear shape, face shape or whiskers in our case of Cat Classification.</p>
						<p>The amount of information improved in the nodes prior to splitting them for further decision-making is the definition of the information gained in the decision tree.</p>
						<img class="figure-img img-fluid shadow rounded" src="Screenshot 2022-10-06 at 12.21.01 AM.png" alt="" title="Information Gain">
						<br><br>
						<p>Let us now see how this equation is used for testing the Decision Tree individually:</p>
						<img class="figure-img img-fluid shadow rounded" src="Screenshot 2022-10-06 at 12.21.34 AM.png" alt="" title="Informatin Gain in Action">
					</section><!--//section-->
					
					<section class="docs-section" id="item-2-3">
						<h2 class="section-heading">Putting it together</h2>
						<p>So, if we take the example Cat Classification and we have features Ear shape, Face shape and Whiskers. Let's look at how the process of Building a Decision Tree is done using the following features.</p>
						<li>Start with all examples at the root node.</li>
						<li>Calculate information gain for all possible features, and pick the one with the highest information gain.</li>
						<li>Split dataset according to selected feature, and create left and right branches of the tree.</li>
						<li>Keep repeating splitting process until stopping criteria is met:</li>
						<li style="margin-left:3em">When a node is 100% one class.</li>
						<li style="margin-left:3em">When splitting a node will result in the tree exceeding a maximum depth.</li>
						<li style="margin-left:3em">Information gain from additional splits is less than threshold.</li>
						<li style="margin-left:3em">When number of examples in a node is below a threshold.</li>
						<br>
						<p>There is another property of Tree data structure, it is called as a <b>Recursive data structure</b>. Recursive algorithm is the way a Decision Tree at the root is by building other smaller decision trees in the left and the right sub-branches.</p>
						<br>
						<p>Let's see the Process through a Gif:</p>
						<img class="figure-img img-fluid shadow rounded" src="ezgif.com-gif-maker copy.gif" alt="" title="Putting it together">
						<br><br>
						<p>If you want to implement the Decision Tree from scratch then the recursive algorithm turns out to be one of the steps that must be implemented.</p>
						<li><b>How to choose the maximum depth parameter?:</b> There are many different possible choices, but some of the open-source libraries will have good default choices that you can use. One intuition is, the larger the maximum depth, the bigger the decision tree you're willing to build. This is a bit like fitting a higher degree polynomial or training a larger neural network. It lets the decision tree learn a more complex model, but it also increases the risk of overfitting if this fitting a very complex function to your data. In theory, you could use cross-validation to pick parameters like the maximum depth, where you try out different values of the maximum depth and pick what works best on the cross-validation set. Although in practice, the open-source libraries have even somewhat better ways to choose this parameter for you. another criteria that you can use to decide when to stop splitting is if the information gained from an additional split is less than a certain threshold. If any feature is split on, achieves only a small reduction in entropy or a very small information gain, then you might also decide to not bother. Finally, you can also decide to start splitting when the number of examples in the node is below a certain threshold. That's the process of building a decision tree.</li>
					</section><!--//section-->

					<section class="docs-section" id="item-2-4">
						<h2 class="section-heading">One Hot Encoding</h2>
						<p>One hot encoding is a process by which categorical variables are converted into a form that could be provided to ML algorithms to do a better job in prediction.</p>
						<img class="figure-img img-fluid shadow rounded" src="https://e6v4p8w2.rocketcdn.me/wp-content/uploads/2022/01/One-Hot-Encoding-for-Scikit-Learn-in-Python-Explained-1024x576.png" alt="" title="One Hot Encoding">
					</section><!--//section-->

					<section class="docs-section" id="item-2-5">
						<h2 class="section-heading">Continuous Valued Features</h2>
						<p>Continuous valued features are the features that are numerical. In our case, Cat classification, we have looked into the process with only 3 features which are categorical ear shape, face shape, and whiskers. If we add a new feature weight of the animal. Then, it is Continuous valued feature.</p>
						<p>We can plot the weights data and find the point where the Information gain will be high in order to present it as a node with feature weight. What we have observed is that <b><code>Weight <= 9</code></b> is a best point where the Informatin gain is very high. So, we choose that point. Let us look at a image to understand more:</p>
						<img class="figure-img img-fluid shadow rounded" src="Screenshot 2022-10-06 at 1.55.41 AM.png" alt="" title="Continuous Valued Features">
					</section><!--//section-->

					<section class="docs-section" id="item-2-6">
						<h2 class="section-heading">Regression Trees</h2>
						<p>Decision Trees is not only used for Classification but it is also used for Regression Analysis also. Let us work on the same example, Cat Classification, here the input features will be categorical features (ear shape, face shape and whiskers) and the output features will be Continuous features (Weight).</p>
						<p>Let us look at diagram to illustrate more on our point: </p>
						<div class="row">
							<div class="column">
								<img class="figure-img img-fluid shadow rounded" src="Screenshot 2022-10-06 at 2.20.43 AM.png" alt="" title="Decision Tree with Regression">
							</div>
							<div class="column">
								<img class="figure-img img-fluid shadow rounded" src="Screenshot 2022-10-06 at 2.21.05 AM.png" alt="" title="Selecting the node for Decision Tree in Regression">
							</div>
						</div>
						<br>
						<li>We will use the same information gain equation but this time we will use Variance instead of entropy for regression analysis for Decision Tree. This provides us the information in our problem that Ear shape has more weightage to be the root node than other features face shape and whiskers.</li>
					</section><!--//section-->
			    </article><!--//docs-article-->
			    
			    
			    <article class="docs-article" id="section-3">
				    <header class="docs-header">
					    <h1 class="docs-heading">Tree Ensembles</h1>
					    <section class="docs-intro">
						    <p>One of the weaknesses of using a single decision tree is that that decision tree can be highly sensitive to small changes in the data. One solution to make the arrow less sensitive or more robust is to build not one decision tree, but to build a lot of decision trees, and we call that a tree ensemble. The fact that changing just one training example causes the algorithm to come up with a different split at the root and therefore a totally different tree, that makes this algorithm just not that robust. That's why when you're using decision trees, you often get a much better result, that is, you get more accurate predictions if you train not just a single decision tree but a whole bunch of different decision trees. This is what we call a tree ensemble, which just means a collection of multiple trees.</p>
							<div class="row">
								<div class="column">
									<img class="figure-img img-fluid shadow rounded" src="Screenshot 2022-10-06 at 2.32.43 AM.png" alt="" title="Tree Ensembles">
								</div>
								<div class="column">
									<img class="figure-img img-fluid shadow rounded" src="Screenshot 2022-10-06 at 2.33.04 AM.png" alt="" title="Tree Ensembles">
								</div>
							</div>
						</section><!--//docs-intro-->
				    </header>

				     <section class="docs-section" id="item-3-1">
						<h2 class="section-heading">Sampling with Replacement</h2>
						<p>As we have discussed the weakness of using a single decision tree is that the decision tree can be highly sensitive to small changes in the data. So, sampling with replacement provides a way to combat this by creating various decision trees with different sets of data with duplicates from the original dataset.</p>
						<p>Let us demonstrate with an example, we take 4 coins of different colours red, blue, yellow, and green. We put all the coins in a bag and shake it.</p>
						<li><b>Step 1:</b> Take a coin from the bag without looking. Let us assume it is red. Then, put the it back into the bag</li>
						<li><b>Step 2:</b> Again take a coin from the bag without looking. Let us assume it is red again. Then, put it back into the bag. As our original set of coins were 4. We would repeat the step of taking and putting it back 4 times.</li>
						<li><b>Step 3:</b> The above 2 steps must be repeated to generate the following datasets:</li>
						<li style="margin-left: 3em;">red, red, yellow, blue (<b>Note:</b> No green.)</li>
						<li style="margin-left: 3em;">red, yellow, blue, and green (We got the original dataset)</li>
						<li style="margin-left: 3em;">yellow, green, green, and green (we got green 3 times)</li>
						<li style="margin-left: 3em;">blue, blue, blue, and blue (we got only blue dataset)</li>
						<p>The above generated datasets using <b>Sampling with Replacement</b> can be used to create different Decision Trees which is the core concept of Tree Ensembles.</p>
						<p>Let us learn about the first Tree Ensemble method: <b>Random Forest Algorithm</b>.</p>
					</section><!--//section-->
					
					<section class="docs-section" id="item-3-2">
						<h2 class="section-heading">Random Forest Algorithm</h2>
						<p>Let us look at a glimpse of Bagged Decision Tree:</p>
						<li>Given training set of size 𝑚</li>
						<li>For 𝑏 = 1 to 𝐵:</li>
						<li style="margin-left:3em">Use sampling with replacement to create a new training set of size 𝑚</li>
						<li style="margin-left:3em">Train a decision tree on the new dataset</li>
						<p>This is algorithm is Bagged Decision Tree where <code>𝑏</code> and <code>𝐵</code> are derived from bag name.</p>
						<li>Random forest is derived from this Bagged Decision Tree. In this process there is voting system at all the prediction produced by bagged trees and predicts based on Voting system. Let us look at a diagram to understand better:</li>
						<li>At each node, when choosing a feature to use to split, if 𝑛 features are available, pick a random subset of 𝑘 < 𝑛 features and allow the algorithm to only choose from that subset of features.</li>
						<li>The best value of k would be <math xmlns="http://www.w3.org/1998/Math/MathML">
							<mstyle displaystyle="true">
							  <mi>k</mi>
							  <mo>=</mo>
							  <msqrt>
								<mi>x</mi>
							  </msqrt>
							</mstyle>
						  </math> .
						  </li>
						<br>
						<img class="figure-img img-fluid shadow rounded" src="https://catalyst.earth/catalyst-system-files/help/COMMON/references/images/RT_schematic.png" alt="" title="Random Forest Tree">
						<br><br>
						<li>In this the Bootstrap sampling is also called as Sampling with Replacement as we have discussed earlier. Bootstrap aggregation is grouping of all the predicted class of different trees.</li>
					</section><!--//section-->
					
					<section class="docs-section" id="item-3-3">
						<h2 class="section-heading">XGBoost</h2>
						<p>XGBoost is a <b>Gradient Boosting Algorithm</b> that is quite popular among Machine learning practitioners. The intuition behind XGBoost is:</p>
						<li>For example, if you're learning to play the piano and you're trying to master a piece on the piano rather than practicing the entire say five minute piece over and over, which is quite time consuming. If you instead play the piece and then focus your attention on just the parts of the piece that you aren't yet playing that well in practice those smaller parts over and over. Then that turns out to be a more efficient way for you to learn to play the piano well. And so this idea of boosting is similar. We're going to look at the decision trees, we've trained so far and look at what we're still not yet doing well on. And then when building the next decision tree, we're going to focus more attention on the examples that we're not yet doing well. So rather than looking at all the training examples, we focus more attention on the subset of examples is not yet doing well on and get the new decision tree, the next decision tree reporting ensemble to try to do well on them. And this is the idea behind boosting and it turns out to help the learning algorithm learn to do better more quickly.</li>
						<p>The change that is done to the previous developed algorithm is that we will give more importance to misclassified examples in order to build the next Decision Tree. The process of giving more importance to misclassified examples is more complex, so don't dwell on it. There are many open-source libraries that are developed to implement XGBoost algorithm.</p>
						<li>Given training set of size 𝑚</li>
						<li>For 𝑏 = 1 to 𝐵:</li>
						<li style="margin-left:3em">Use sampling with replacement to create a new training set of size 𝑚</li>
						<p style="margin-left:6em">But instead of picking from all examples with equal (1/m) probability, make it more likely to pick examples that the previously trained trees misclassify.</p>
						<li style="margin-left:3em">Train a decision tree on the new dataset</li>

						<br>
						<p>Some advantages of using XGBoost algorithms are:</p>
						<li>Open source implementation of boosted trees.</li>
						<li>Fast efficient implementation.</li>
						<li>Good choice of default splitting criteria and criteria for when to stop splitting.</li>
						<li>Built in regularization to prevent overfitting.</li>
						<li>Highly competitive algorithm for machine learning competitions (eg: Kaggle competitions).</li>
<br>
						<h3>Implementation of XGBoost</h3>
						<div class="row">
							<div class="column">
								<h4>Classification</h4>
								<div class="docs-code-block">
									<pre class="shadow-lg rounded"><code class="language-python">
from xgboost import XGBClassifier
model = XGBClassifier()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
	</code></pre>
								</div><!--//docs-code-block-->
							</div>
							<div class="column">
								<h4>Regression</h4>
								<div class="docs-code-block">
									<pre class="shadow-lg rounded"><code class="language-python">
from xgboost import XGBRegressor
model = XGBRegressor()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
	</code></pre>
								</div><!--//docs-code-block-->
							</div>
						</div>
					</section><!--//section-->

					<section class="docs-section" id="item-3-4">
						<h2 class="section-heading">When to use Decision Tree?</h2>
						<p>There is always a doubt of when to use Decision Trees and when to use Neural Network. Let us first understand the advantages and disadvantages of each of them:</p>
						<br><h3>Decision Tree and Tree Ensembles</h3>
						<p>Let us discuss some key points:</p>
						<li>Works well on tabular (structured) data.</li>
						<li>Not recommended for unstructured data (images, audio, text).</li>
						<li>Fast, even the ensemble methods are faster than regular Neural Networks.</li>
						<li>Small decision trees may be human interpretable.</li>
						<br><h3>Neural Network</h3>
						<li>Works well on all types of data, including tabular (structured) and unstructured data.</li>
						<li>May be slower than a decision tree. When you consider the Machine Learning Developement cycle. It takes a lot time to enter the Diagnostics stage which will slow down the entire cycle. For Structured data it would be better to create a Decision Tree as it is fast.</li>
						<li>Works with transfer learning.</li>
						<li>When building a system of multiple models working together, it might be easier to string together multiple neural networks.</li>
					</section><!--//section-->
			    </article><!--//docs-article-->
			    
			    
			    <article class="docs-article" id="section-4">
				    <header class="docs-header">
					    <h1 class="docs-heading">Conclusion</h1>
					    <section class="docs-intro">
						    <p>This is the end of Course 2 of Machine Learning Specialization. Let's enter the Course 3 of MLS.</p>
						</section><!--//docs-intro-->
				    </header>
				     <section class="docs-section" id="item-4-1">
						<h2 class="section-heading">What we have covered?</h2>
						<li>Learnt about Decision Trees.</li>
						<li>Learnt about Entropy and Information Gain in Decision Tree.</li>
						<li>Learnt about Tree Ensembles techniques.</li>
						<li>Learnt about Random Forest Algorithm.</li>
						<li>Learnt about XGBoost and its importance.</li>
						<li>Learnt about When to use Decision Trees?</li>
					</section><!--//section-->
			    </article><!--//docs-article-->

			    <footer class="footer">
				    <div class="container text-center py-5">
				        <!--/* This template is free as long as you keep the footer attribution link. If you'd like to use the template without the attribution link, you can buy the commercial license via our website: themes.3rdwavemedia.com Thank you for your support. :) */-->
			            <!-- <small class="copyright">Designed with <span class="sr-only">love</span><i class="fas fa-heart" style="color: #fb866a;"></i> by <a class="theme-link" href="http://themes.3rdwavemedia.com" target="_blank">Xiaoying Riley</a> for developers</small> -->
				        <ul class="social-list list-unstyled pt-4 mb-0">
						    <li class="list-inline-item"><a href="https://github.com/bhanuprasanna527" target="_blank" rel="noopener noreferrer"><i class="fab fa-github fa-fw"></i></a></li>
			            <li class="list-inline-item"><a href="https://twitter.com/bhanu527" target="_blank" rel="noopener noreferrer"><i class="fab fa-twitter fa-fw"></i></a></li>
		                <li class="list-inline-item"><a href="https://www.linkedin.com/in/bhanu-prasanna-a1b71b1a2/" target="_blank" rel="noopener noreferrer"><i class="fab fa-linkedin fa-fw"></i></a></li>
		                <li class="list-inline-item"><a href="https://www.instagram.com/bhanuprasanna527/" target="_blank" rel="noopener noreferrer"><i class="fab fa-instagram fa-fw"></i></a></li>
				        </ul><!--//social-list-->
				    </div>
			    </footer>
		    </div> 
	    </div>
    </div><!--//docs-wrapper-->
   
       
    <!-- Javascript -->          
    <script src="/assets/plugins/popper.min.js"></script>
    <script src="/assets/plugins/bootstrap/js/bootstrap.min.js"></script>  
    
    
    <!-- Page Specific JS -->
    <script src="/assets/plugins/smoothscroll.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.8/highlight.min.js"></script>
    <script src="/assets/js/highlight-custom.js"></script> 
    <script src="/assets/plugins/simplelightbox/simple-lightbox.min.js"></script>      
    <script src="/assets/plugins/gumshoe/gumshoe.polyfills.min.js"></script> 
    <script src="/assets/js/docs.js"></script> 

</body>
</html> 

