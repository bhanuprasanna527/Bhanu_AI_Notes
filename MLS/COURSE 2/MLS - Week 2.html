<!DOCTYPE html>
<html lang="en"> 
<head>
    <title>Bhanu.AI Machine Learning Specialization</title>
    
    <!-- Meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="AI Notes written by Bhanu Prasanna from Various Specializations.">
    <meta name="author" content="Bhanu prasanna">    
    <link rel="shortcut icon" href="/favicon.ico">

	<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    <!-- Google Font -->
    <link href="https://fonts.googleapis.com/css?family=Poppins:300,400,500,600,700&display=swap" rel="stylesheet">
    
    <!-- FontAwesome JS-->
    <script defer src="/assets/fontawesome/js/all.min.js"></script>
    
    <!-- Plugins CSS -->
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.2/styles/atom-one-dark.min.css">
    <link rel="stylesheet" href="/assets/plugins/simplelightbox/simple-lightbox.min.css">

    <!-- Theme CSS -->  
    <link id="theme-style" rel="stylesheet" href="/assets/css/theme.css">

</head> 

<body class="docs-page">    
    <header class="fixed-top">	    
        <div class="branding docs-branding">
            <div class="container-fluid position-relative py-2">
                <div class="docs-logo-wrapper">
					<button id="docs-sidebar-toggler" class="docs-sidebar-toggler docs-sidebar-visible me-2 d-xl-none" type="button">
	                    <span></span>
	                    <span></span>
	                    <span></span>
	                </button>
	                <div class="site-logo"><a class="navbar-brand" href="/index.html"><img class="logo-icon me-2" src="/assets/images/coderdocs-logo.svg" alt="logo"><span class="logo-text">Bhanu<span class="text-alt">.AI</span></span></a></div>    
                </div><!--//docs-logo-wrapper-->
	            <div class="docs-top-utilities d-flex justify-content-end align-items-center">
	                <div class="top-search-box d-none d-lg-flex">
		                <form class="search-form">
				            <input type="text" placeholder="Search the docs..." name="search" class="form-control search-input">
				            <button type="submit" class="btn search-btn" value="Search"><i class="fas fa-search"></i></button>
				        </form>
	                </div>
	
					<ul class="social-list list-inline mx-md-3 mx-lg-5 mb-0 d-none d-lg-flex">
						<li class="list-inline-item"><a href="https://github.com/bhanuprasanna527" target="_blank" rel="noopener noreferrer"><i class="fab fa-github fa-fw"></i></a></li>
			            <li class="list-inline-item"><a href="https://twitter.com/bhanu527" target="_blank" rel="noopener noreferrer"><i class="fab fa-twitter fa-fw"></i></a></li>
		                <li class="list-inline-item"><a href="https://www.linkedin.com/in/bhanu-prasanna-a1b71b1a2/" target="_blank" rel="noopener noreferrer"><i class="fab fa-linkedin fa-fw"></i></a></li>
		                <li class="list-inline-item"><a href="https://www.instagram.com/bhanuprasanna527/" target="_blank" rel="noopener noreferrer"><i class="fab fa-instagram fa-fw"></i></a></li>
		            </ul><!--//social-list-->
		            <a href="https://github.com/bhanuprasanna527/Bhanu_AI_Notes" class="btn btn-primary d-none d-lg-flex" target="_blank" rel="noopener noreferrer">GITHUB 🌟</a>
	            </div><!--//docs-top-utilities-->
            </div><!--//container-->
        </div><!--//branding-->
    </header><!--//header-->
    
    
    <div class="docs-wrapper">
	    <div id="docs-sidebar" class="docs-sidebar">
		    <div class="top-search-box d-lg-none p-3">
                <form class="search-form">
		            <input type="text" placeholder="Search the docs..." name="search" class="form-control search-input">
		            <button type="submit" class="btn search-btn" value="Search"><i class="fas fa-search"></i></button>
		        </form>
            </div>
		    <nav id="docs-nav" class="docs-nav navbar">
			    <ul class="section-items list-unstyled nav flex-column pb-3">
				    <li class="nav-item section-title"><a class="nav-link scrollto active" href="#section-1"><span class="theme-icon-holder me-2"><i class="fa-solid fa-1"></i></span>Neural Network Training</a></li>
				    <li class="nav-item"><a class="nav-link scrollto" href="#item-1-1">Neural Network Code</a></li>
				    <li class="nav-item section-title mt-3"><a class="nav-link scrollto" href="#section-2"><span class="theme-icon-holder me-2"><i class="fa-solid fa-2"></i></span>Activation Function</a></li>
				    <li class="nav-item"><a class="nav-link scrollto" href="#item-2-1">Implement Activation Function</a></li>
				    <li class="nav-item section-title mt-3"><a class="nav-link scrollto" href="#section-3"><span class="theme-icon-holder me-2"><i class="fa-solid fa-3"></i></span>Alternative to Sigmoid Function</a></li>
				    <li class="nav-item"><a class="nav-link scrollto" href="#item-3-1">Output Layer</a></li>
				    <li class="nav-item"><a class="nav-link scrollto" href="#item-3-2">Hidden Layers</a></li>
				    <li class="nav-item section-title mt-3"><a class="nav-link scrollto" href="#section-4"><span class="theme-icon-holder me-2"><i class="fa-solid fa-4"></i></span>Purpose of Activation Function</a></li>
				    <li class="nav-item"><a class="nav-link scrollto" href="#item-4-1">2 Challenges of Training</a></li>
				    <li class="nav-item"><a class="nav-link scrollto" href="#item-4-2">Choosing an Activation Function</a></li>
				    <li class="nav-item section-title mt-3"><a class="nav-link scrollto" href="#section-5"><span class="theme-icon-holder me-2"><i class="fa-solid fa-5"></i></span>Multi-class Classification</a></li>
				    <li class="nav-item"><a class="nav-link scrollto" href="#item-5-1">Model</a></li>
				    <li class="nav-item"><a class="nav-link scrollto" href="#item-5-2">Model Details</a></li>
				    <li class="nav-item section-title mt-3"><a class="nav-link scrollto" href="#section-6"><span class="theme-icon-holder me-2"><i class="fa-solid fa-6"></i></span>Softmax Activation Function</a></li>
				    <li class="nav-item"><a class="nav-link scrollto" href="#item-6-1">Softmax Equation</a></li>
				    <li class="nav-item"><a class="nav-link scrollto" href="#item-6-2">Cost</a></li>
				    <li class="nav-item section-title mt-3"><a class="nav-link scrollto" href="#section-7"><span class="theme-icon-holder me-2"><i class="fa-solid fa-7"></i></span>Adam Optimizer</a></li>
				    <li class="nav-item"><a class="nav-link scrollto" href="#item-7-1">Points about Adam</a></li>
				    <li class="nav-item section-title mt-3"><a class="nav-link scrollto" href="#section-8"><span class="theme-icon-holder me-2"><i class="fa-solid fa-8"></i></span>Conclusion</a></li>
				    <li class="nav-item"><a class="nav-link scrollto" href="#item-8-1">What we have covered?</a></li>
			    </ul>

		    </nav><!--//docs-nav-->
	    </div><!--//docs-sidebar-->
	    <div class="docs-content">
		    <div class="container">
			    <article class="docs-article" id="section-1">
				    <header class="docs-header">
					    <h1 class="docs-heading">Neural Network Training<span class="docs-time">Last updated: 2019-06-01</span></h1>
					    <section class="docs-intro">
						    <p>In this course, we have used Tensorflow as the library to create Neural Networks. But, what to do if we don't understand what we are writing to create a Neural Network.</p>
							<li>In this week we will understand what does <code>model.compile()</code> and <code>model.fit()</code> mean in Tensorflow and how it enables to create a working Neural Network.</li>
							<li>We will learn about other Activation Function and Optimisation Algorithms used in Neural Network.</li>
						</section><!--//docs-intro-->
						
				    </header>
				    <section class="docs-section" id="item-1-1">
						<h2 class="section-heading">Neural Network Code</h2>
						<p>The below code specifies the model used for MNIST digit recognition of only two digits (0 and 1).</p>
						<div class="docs-code-block">
							<pre class="shadow-lg rounded"><code class="language-python">
import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense
model = Sequential([
		Dense(units=25, activation='sigmoid'),
		Dense(units=15, activation='sigmoid'),
		Dense(units=1, activation='sigmoid')
			])
from tensorflow.keras.losses import BinaryCrossentropy
model.compile(loss=BinaryCrossentropy())
model.fit(X,Y,epochs=100)
							</code></pre>
						</div><!--//docs-code-block-->
						<p>We will take individual parts of the code written above and try to explain in detail.</p>
						<h3>1 -> Define Model</h3>
						<p>In logistic regression using NumPy we would define the model by specifying how to compute the output using the given input x and parameters w, b.</p>
						<div class="row">
							<div class="column">
								<h4>Logistic Regression</h4>
								<div class="docs-code-block">
									<pre class="shadow-lg rounded"><code class="language-python">
z = np.dot(w, x) + b
f_x = 1 / (1 + np.exp(-z))
	</code></pre>
								</div><!--//docs-code-block-->
							</div>

							<div class="column">
								<h4>Neural Network</h4>
								<div class="docs-code-block">
									<pre class="shadow-lg rounded"><code class="language-python">
model = Sequential([
		Dense(...) 
		Dense(...) 
		Dense(...)
			])
	</code></pre>
								</div><!--//docs-code-block-->
							</div>

					</div>

					<h3>2 -> Specify Loss and Cost</h3>
					<p>In this we will see how to specify Loss and Cost:</p>
					<div class="row">
						<div class="column">
							<h4>Logistic Loss</h4>
							<div class="docs-code-block">
								<pre class="shadow-lg rounded"><code class="language-python">
loss = -y * np.log(f_x) -(1-y) * np.log(1-f_x)
			</code></pre>
							</div>
						</div>
						<div class="column">
							<h4>Binary Cross Entropy</h4>
							<div class="docs-code-block">
								<pre class="shadow-lg rounded"><code class="language-python">
model.compile(loss=BinaryCrossentropy())
			</code></pre>
							</div>
						</div>
					</div>

					<h3>3 -> Train on data to minimize cost</h3>
					<p>In this we will see how to train on data to minimize cost:</p>
					<div class="row">
						<div class="column">
							<h4>Logistic Loss</h4>
							<div class="docs-code-block">
								<pre class="shadow-lg rounded"><code class="language-python">
w = w - alpha * dj_dw
b = b - alpha * dj_db
		</code></pre>
							</div>
						</div>
						<div class="column">
							<h4>Binary Cross Entropy</h4>
							<div class="docs-code-block">
								<pre class="shadow-lg rounded"><code class="language-python">
model.fit(X,y,epochs=100)
			</code></pre>
							</div>
						</div>
					</div>
					</section><!--//section-->
			    </article>
			    
			    <article class="docs-article" id="section-2">
				    <header class="docs-header">
					    <h1 class="docs-heading">Activation Function</h1>
					    <section class="docs-intro">
						    <p>The activation function defines the output of a neuron / node given an input or set of input (output of multiple neurons).</p>
							<p>There are many types of activation function used in Neural Networks.</p>
							<img class="figure-img img-fluid shadow rounded" src="https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/62b18a8dc83132e1a479b65d_neural-network-activation-function-cheat-sheet.jpeg" alt="" title="Types of Activation Function">
							<br><br><li>To learn more about Activation Functions visit this Notebook <a href="MLS - MODEL AF.html"><b>LINK</b></a>.</li>
							<li>For more information about Activation Functions visit <a href="https://www.v7labs.com/blog/neural-networks-activation-functions"><b>LINK</b></a> and <a href="https://towardsdatascience.com/what-is-activation-function-1464a629cdca"><b>LINK</b></a>.</li>
						</section><!--//docs-intro-->
				    </header>
				     <section class="docs-section" id="item-2-1">
						<h2 class="section-heading">Implement Activation Function</h2>
						<p>Here we will show how the implementatin of various activation functions with equations associated with it.</p>
						<li>To learn more about Activation Functions visit this Notebook <a href="MLS - MODEL AF.html" style="font-size: large;"><b>LINK</b></a>.</li>
					</section><!--//section-->
				
			    </article><!--//docs-article-->
			    
			    
			    <article class="docs-article" id="section-3">
				    <header class="docs-header">
					    <h1 class="docs-heading">Alternative to Sigmoid Function</h1>
					    <section class="docs-intro">
						    <p>Usage of Sigmoid Activation Function sometimes takes a plunge at the efficiency of our model. So, the usage of sigmoid activation function must be avoided.</p>
							<li>Let us understand the usage of Activation Function in Output and Hidden Layers.</li>
						</section><!--//docs-intro-->
				    </header>
				     <section class="docs-section" id="item-3-1">
						<h2 class="section-heading">Output Layer</h2>
						<p>Let us observe how certain activation funtions are used in output layer in certain instance:</p>
						<h3>Stock Price Prediction</h3>
						<p>Here we might use a Linear Activation function so as to tackle negative values.</p>
						<li>Here the output values can be negative or positive. So, we need a activation function that can tackle both Positive and Negative values together.</li>
						<li>We know that Linear activation takes consideration all types of values. So, linear activation function can tackle this problem at the output layer.</li>
						<li>Other Activation function like ReLU can only tackle Positive changes in values which is a drawback.</li>
						<br>
						<h3>Buy/ Not Buy</h3>
						<p>This is Binary Classification problem where the customer can buy the product or not.</p>
						<li>Here we can use sigmoid activation function to tackle as it provides values in the range of <code>0 or 1</code></li>
						<br>
						<h3>House Price Prediction</h3>
						<p>This is also a Regression Problem where we would like to predict the price of house.</p>
						<li>Here we can use ReLU activation function to tackle as the output values are non-negative values and we know that ReLU can only tackle Positive values.</li>
					</section><!--//section-->
					
					<section class="docs-section" id="item-3-2">
						<h2 class="section-heading">Hidden Layer</h2>
						<li>In hidden layer, the most common practice is to use ReLU activation function.</li>
						<li>ReLU Activation function is used so much is because it helps the model to train faster than regular activation fucntions.</li>
						<li>Other activation functions can also be used in hidden layer. It is mostly based on the type of problem that we are dealing with.</li>
					</section><!--//section-->
			    </article><!--//docs-article-->
			    
			    <article class="docs-article" id="section-4">
				    <header class="docs-header">
					    <h1 class="docs-heading">Purpose of Activation Function</h1>
					    <section class="docs-intro">
						    <p>The sole purpose of an activation function is to add non-linearity to the neural network. Activation functions introduce an additional step at each layer during the forward propagation, but its computation is worth it.</p>
							<p>Let’s suppose we have a neural network working without the activation functions. </p>
							<li>In that case, every neuron will only be performing a linear transformation on the inputs using the weights and biases. It’s because it doesn’t matter how many hidden layers we attach in the neural network; all layers will behave in the same way because the composition of two linear functions is a linear function itself.</li>
							<li>Although the neural network becomes simpler, learning any complex task is impossible, and our model would be just a linear regression model.</li>
						</section><!--//docs-intro-->
				    </header>
				     <section class="docs-section" id="item-4-1">
						<h2 class="section-heading">2 Challenges of Training</h2>
						<p>There are 2 challenges associated to difficulty of training a Neural Network: </p>
						<h3>Vanishing Gradients</h3>
						<li>Some activation functions, like the sigmoid function, compress a large input space into a relatively small output space between 0 and 1.</li>
						<li>As a result, the output of the sigmoid function will only change slightly if the input changes significantly.As a result, the derivative shrinks.This isn't a big problem for networks with few layers that use these activations.</li>
						<li>However, training may not function properly if the gradient is too small when there are more layers used.</li>

						<br>
						<h3>Exploding Gradients</h3>
						<li>Exploding gradients are issues in which large updates to neural network model weights during training are brought about by the accumulation of significant error gradients.</li>
						<li>Exploding gradients can lead to an unstable network that prevents learning from being completed.</li>
						<li>Weights' values can also become so large that they overflow, resulting in so-called NaN values.</li>
					</section><!--//section-->
					
					<section class="docs-section" id="item-4-2">
						<h2 class="section-heading">Choosing an Activation Function</h2>
						<p>A neural network will almost always have the same activation function in all hidden layers. It is most unusual to vary the activation function through a network model.</p>
						<p>Some common guidelines about Activation Function:</p>
						<li>Only the hidden layers should make use of the ReLU activation function.</li>
						<li>Due to vanishing gradients, the Sigmoid/Logistic and Tanh functions should not be used in hidden layers because they make the model more susceptible to issues during training.</li>
						<li>Swish function is used in neural networks having a depth greater than 40 layers.</li>
						<br>
						<p>Finally, a few rules for choosing the activation function for your output layer based on the type of prediction problem that you are solving:</p>
						<ol>
							<li><b>Regression:</b> Linear Activation Function</li>
							<li><b>Binary Classification:</b> Sigmoid/Logistic Activation Function</li>
							<li><b>Multiclass Classification:</b> Softmax</li>
							<li><b>Multilabel Classification:</b> Sigmoid</li>
							<li><b>Convolutional Neural Network (CNN):</b> ReLU activation function</li>
							<li><b>Recurrent Neural Network:</b> Tanh and/or Sigmoid activation function</li>
						</ol>
						<p>If you’re unsure which activation function to use for your network, try a few and compare the results.</p>
					</section><!--//section-->
			    </article><!--//docs-article-->
			    
			    <article class="docs-article" id="section-5">
				    <header class="docs-header">
					    <h1 class="docs-heading">Multi-class Classification</h1>
					    <section class="docs-intro">
						    <p>Neural Networks are often used to classify data. Examples are neural networks:</p>
							<li>Take in photos and classify subjects in the photos as {dog,cat,horse,other}.</li>
							<li>Take in a sentence and classify the 'parts of speech' of its elements: {noun, verb, adjective etc..}.</li>
							<p>A network of this type will have multiple units in its final layer. Each output is associated with a category. When an input example is applied to the network, the output with the highest value is the category predicted. If the output is applied to a softmax function, the output of the softmax will provide probabilities of the input being in each category.</p>
						</section><!--//docs-intro-->
				    </header>
				     <section class="docs-section" id="item-5-1">
						<h2 class="section-heading">Model</h2>
						<p>Model for Multi-class Classification is:</p>
						<div class="docs-code-block">
							<pre class="shadow-lg rounded"><code class="language-python">
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

model = Sequential(
    [
        Dense(2, activation = 'relu',   name = "L1"),
        Dense(4, activation = 'linear', name = "L2")
    ]
)

model.compile(
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    optimizer=tf.keras.optimizers.Adam(0.01),
)

model.fit(
    X_train,y_train,
    epochs=200
)	
							</code></pre>
						</div>
					</section><!--//section-->
					
					<section class="docs-section" id="item-5-2">
						<h2 class="section-heading">Model Details</h2>
						<p>Here we will discuss about each aspect of model present in code:</p>
						<br>
						<h3>Importing Libraries</h3>
						<p>Here we used Tensorflow Library for creating a multi-class classification model.</p>
						<br>
						<h3>Defining Model</h3>
						<p>Here we created a Sequential model using Tensorflow.</p>
						<p>Sequential model consists of 2 layers:</p>
						<li>First Dense layer with activation function ReLU (REctified Linear Unit).</li>
						<li>Second Dense layer with activation function Linear.</li>
						<br>
						<h3>Compiling Model</h3>
						<p>This phase consists of Compiling the model:</p>
						<li>We choose the Loss function as SparseCategoricalCrossentropy.</li>
						<li>The <code>from_logits=True</code> is used for perfecting the Numerical errors established in Python. Tensorflow has created this to correct the Numerical errors.</li>
						<li>We choose the optimizer to be Adam optimizer. We will discuss more about Adam below.</li>
					</section><!--//section-->
			    </article><!--//docs-article-->
			    
			    
		        <article class="docs-article" id="section-6">
				    <header class="docs-header">
					    <h1 class="docs-heading">Softmax Activation Function</h1>
					    <section class="docs-intro">
						    <p>In both softmax regression and neural networks with Softmax outputs, N outputs are generated and one output is selected as the predicted category. In both cases a vector  𝐳  is generated by a linear function which is applied to a softmax function. The softmax function converts  𝐳  into a probability distribution as described below. After applying softmax, each output will be between 0 and 1 and the outputs will add to 1, so that they can be interpreted as probabilities. The larger inputs will correspond to larger output probabilities.</p>
							<center>  <img class="figure-img img-fluid shadow rounded" src="C2_W2_SoftmaxReg_NN.png" alt="" title="Softmax Function">  </center>
						</section><!--//docs-intro-->
				    </header>
				     <section class="docs-section" id="item-6-1">
						<h2 class="section-heading">Softmax Equation</h2>
						<p>Softmax function is:</p>
						<p><math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
							<semantics>
							  <mtable displaystyle="true">
								<mlabeledtr>
								  <mtd id="mjx-eqn-1">
									<mtext>(1)</mtext>
								  </mtd>
								  <mtd>
									<msub>
									  <mi>a</mi>
									  <mi>j</mi>
									</msub>
									<mo>=</mo>
									<mfrac>
									  <msup>
										<mi>e</mi>
										<mrow class="MJX-TeXAtom-ORD">
										  <msub>
											<mi>z</mi>
											<mi>j</mi>
										  </msub>
										</mrow>
									  </msup>
									  <mrow>
										<munderover>
										  <mo>&#x2211;<!-- ∑ --></mo>
										  <mrow class="MJX-TeXAtom-ORD">
											<mi>k</mi>
											<mo>=</mo>
											<mn>1</mn>
										  </mrow>
										  <mrow class="MJX-TeXAtom-ORD">
											<mi>N</mi>
										  </mrow>
										</munderover>
										<mrow class="MJX-TeXAtom-ORD">
										  <msup>
											<mi>e</mi>
											<mrow class="MJX-TeXAtom-ORD">
											  <msub>
												<mi>z</mi>
												<mi>k</mi>
											  </msub>
											</mrow>
										  </msup>
										</mrow>
									  </mrow>
									</mfrac>
								  </mtd>
								</mlabeledtr>
							  </mtable>
							  <annotation encoding="application/x-tex">a_j = \frac{e^{z_j}}{ \sum_{k=1}^{N}{e^{z_k} }} \tag{1}</annotation>
							</semantics>
						  </math>
						  </p>
						  <p>The output  𝐚  is a vector of length N, so for softmax regression, you could also write:</p>
						  <p><math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
							<mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true">
							  <mlabeledtr>
								<mtd id="mjx-eqn-2">
								  <mtext>(2)</mtext>
								</mtd>
								<mtd>
										<mi mathvariant="bold">a</mi>
								  <mo stretchy="false">(</mo>
								  <mi>x</mi>
								  <mo stretchy="false">)</mo>
								  <mo>=</mo>
								  <mrow>
									<mo>[</mo>
									<mtable rowspacing="4pt" columnspacing="1em">
									  <mtr>
										<mtd>
										  <mi>P</mi>
										  <mo stretchy="false">(</mo>
										  <mi>y</mi>
										  <mo>=</mo>
										  <mn>1</mn>
														<mo stretchy="false">|</mo>
														<mi mathvariant="bold">x</mi>
										  <mo>;</mo>
														<mi mathvariant="bold">w</mi>
										  <mo>,</mo>
										  <mi>b</mi>
										  <mo stretchy="false">)</mo>
										</mtd>
									  </mtr>
									  <mtr>
										<mtd>
										  <mo>&#x22EE;<!-- ⋮ --></mo>
										</mtd>
									  </mtr>
									  <mtr>
										<mtd>
										  <mi>P</mi>
										  <mo stretchy="false">(</mo>
										  <mi>y</mi>
										  <mo>=</mo>
										  <mi>N</mi>
														<mo stretchy="false">|</mo>
														<mi mathvariant="bold">x</mi>
										  <mo>;</mo>
														<mi mathvariant="bold">w</mi>
										  <mo>,</mo>
										  <mi>b</mi>
										  <mo stretchy="false">)</mo>
										</mtd>
									  </mtr>
									</mtable>
									<mo>]</mo>
								  </mrow>
								  <mo>=</mo>
								  <mfrac>
									<mn>1</mn>
									<mrow>
									  <munderover>
										<mo>&#x2211;<!-- ∑ --></mo>
										<mrow>
										  <mi>k</mi>
										  <mo>=</mo>
										  <mn>1</mn>
										</mrow>
													<mi>N</mi>
									  </munderover>
												<msup>
										<mi>e</mi>
													<msub>
										  <mi>z</mi>
										  <mi>k</mi>
										</msub>
									  </msup>
									</mrow>
								  </mfrac>
								  <mrow>
									<mo>[</mo>
									<mtable rowspacing="4pt" columnspacing="1em">
									  <mtr>
										<mtd>
										  <msup>
											<mi>e</mi>
															<msub>
											  <mi>z</mi>
											  <mn>1</mn>
											</msub>
										  </msup>
										</mtd>
									  </mtr>
									  <mtr>
										<mtd>
										  <mo>&#x22EE;<!-- ⋮ --></mo>
										</mtd>
									  </mtr>
									  <mtr>
										<mtd>
										  <msup>
											<mi>e</mi>
															<msub>
											  <mi>z</mi>
																<mi>N</mi>
											</msub>
										  </msup>
										</mtd>
									  </mtr>
									</mtable>
									<mo>]</mo>
								  </mrow>
								</mtd>
							  </mlabeledtr>
							</mtable>
						  </math>
						  </p>

					</section><!--//section-->
					
					<section class="docs-section" id="item-6-2">
						<h2 class="section-heading">Cost</h2>
						<p>The loss function associated with Softmax, the cross-entropy loss, is:</p>
						<p>$$\begin{equation}
							L(\mathbf{a},y)=\begin{cases}
							  -log(a_1), & \text{if $y=1$}.\\
								  &\vdots\\
							   -log(a_N), & \text{if $y=N$}
							\end{cases} \tag{3}
						  \end{equation}$$</p>
						  <p>Where y is the target category for this example and  𝐚  is the output of a softmax function. In particular, the values in  𝐚  are probabilities that sum to one.</p>
						  <li>Loss is for one example while Cost covers all examples.</li>
						  <p>Note in (3) above, only the line that corresponds to the target contributes to the loss, other lines are zero. To write the cost equation we need an 'indicator function' that will be 1 when the index matches the target and zero otherwise.</p>
						  <p>$$\mathbf{1}\{y == n\} = =\begin{cases}
							1, & \text{if $y==n$}.\\
							0, & \text{otherwise}.
						  \end{cases}$$</p>
						  <p>Now the cost is:</p>
						  <p>$$\begin{align}
							J(\mathbf{w},b) = -\frac{1}{m} \left[ \sum_{i=1}^{m} \sum_{j=1}^{N}  1\left\{y^{(i)} == j\right\} \log \frac{e^{z^{(i)}_j}}{\sum_{k=1}^N e^{z^{(i)}_k} }\right] \tag{4}
							\end{align}$$</p>
							<li>Where  𝑚  is the number of examples,  𝑁  is the number of outputs. This is the average of all the losses.</li>
					</section><!--//section-->
			    </article><!--//docs-article-->
			    
			    
			    <article class="docs-article" id="section-7">
				    <header class="docs-header">
					    <h1 class="docs-heading">Adam Optimizer</h1>
					    <section class="docs-intro">
						    <p>Adam is an optimization algorithm that can be used instead of the classical stochastic gradient descent procedure to update network weights iterative based in training data.</p>
							<p>The Adam optimization algorithm is an extension to stochastic gradient descent that has recently seen broader adoption for deep learning applications in computer vision and natural language processing.</p>
						</section><!--//docs-intro-->
				    </header>
					<section class="docs-section" id="item-7-1">
						<h2 class="section-heading">Points about Adam</h2>
						<p>Points about how Adam Optimizer works:</p>
							<li>If the learning rate provided is large or it oscillates at a large rate. Then, Adam optimizer reduces the learning rate to slow down or to decrease the size of the steps taken to reach Global Minimum.</li>
							<li>If the learning rate provided is too small or it takes tiny steps at each interval. Then, Adam optimizer increases the learning rate to reach faster to Global Minimum.</li>
					</section><!--//section-->
			    </article><!--//docs-article-->
			    
			    
			    <article class="docs-article" id="section-8">
				    <header class="docs-header">
					    <h1 class="docs-heading">Conclusion</h1>
					    <section class="docs-intro">
						    <p>This is the end of Week 2 of Machine learning specialization. Let's go to Week 3 of MLS.</p>
						</section><!--//docs-intro-->
				    </header>
				     <section class="docs-section" id="item-8-1">
						<h2 class="section-heading">What we have covered?</h2>
						<li>Learnt about Neural Network Training</li>
						<li>Learnt in detail about individual methods used for model</li>
						<li>Looked into various Activation Functions</li>
						<li>Understood the purpose of Activation Function</li>
						<li>Understood How to choose Activation Function</li>
						<li>Looked into Softmax and Adam in depth</li>
					</section><!--//section-->
			    </article><!--//docs-article-->
			    

			    <footer class="footer">
				    <div class="container text-center py-5">
				        <!--/* This template is free as long as you keep the footer attribution link. If you'd like to use the template without the attribution link, you can buy the commercial license via our website: themes.3rdwavemedia.com Thank you for your support. :) */-->
			            <!-- <small class="copyright">Designed with <span class="sr-only">love</span><i class="fas fa-heart" style="color: #fb866a;"></i> by <a class="theme-link" href="http://themes.3rdwavemedia.com" target="_blank">Xiaoying Riley</a> for developers</small> -->
				        <ul class="social-list list-unstyled pt-4 mb-0">
						    <li class="list-inline-item"><a href="https://github.com/bhanuprasanna527" target="_blank" rel="noopener noreferrer"><i class="fab fa-github fa-fw"></i></a></li>
			            <li class="list-inline-item"><a href="https://twitter.com/bhanu527" target="_blank" rel="noopener noreferrer"><i class="fab fa-twitter fa-fw"></i></a></li>
		                <li class="list-inline-item"><a href="https://www.linkedin.com/in/bhanu-prasanna-a1b71b1a2/" target="_blank" rel="noopener noreferrer"><i class="fab fa-linkedin fa-fw"></i></a></li>
		                <li class="list-inline-item"><a href="https://www.instagram.com/bhanuprasanna527/" target="_blank" rel="noopener noreferrer"><i class="fab fa-instagram fa-fw"></i></a></li>
				        </ul><!--//social-list-->
				    </div>
			    </footer>
		    </div> 
	    </div>
    </div><!--//docs-wrapper-->
   
       
    <!-- Javascript -->          
    <script src="/assets/plugins/popper.min.js"></script>
    <script src="/assets/plugins/bootstrap/js/bootstrap.min.js"></script>  
    
    
    <!-- Page Specific JS -->
    <script src="/assets/plugins/smoothscroll.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.8/highlight.min.js"></script>
    <script src="/assets/js/highlight-custom.js"></script> 
    <script src="/assets/plugins/simplelightbox/simple-lightbox.min.js"></script>      
    <script src="/assets/plugins/gumshoe/gumshoe.polyfills.min.js"></script> 
    <script src="/assets/js/docs.js"></script> 

</body>
</html> 

