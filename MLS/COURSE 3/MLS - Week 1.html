<!DOCTYPE html>
<html lang="en"> 
<head>
    <title>Bhanu.AI Machine Learning Specialization</title>
    
    <!-- Meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="AI Notes written by Bhanu Prasanna from Various Specializations.">
    <meta name="author" content="Bhanu prasanna">    
    <link rel="shortcut icon" href="/favicon.ico">
	<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    <!-- Google Font -->
    <link href="https://fonts.googleapis.com/css?family=Poppins:300,400,500,600,700&display=swap" rel="stylesheet">
    
    <!-- FontAwesome JS-->
    <script defer src="/assets/fontawesome/js/all.min.js"></script>
    
    <!-- Plugins CSS -->
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.2/styles/atom-one-dark.min.css">
    <link rel="stylesheet" href="/assets/plugins/simplelightbox/simple-lightbox.min.css">

    <!-- Theme CSS -->  
    <link id="theme-style" rel="stylesheet" href="/assets/css/theme.css">

</head> 

<body class="docs-page">    
    <header class="fixed-top">	    
        <div class="branding docs-branding">
            <div class="container-fluid position-relative py-2">
                <div class="docs-logo-wrapper">
					<button id="docs-sidebar-toggler" class="docs-sidebar-toggler docs-sidebar-visible me-2 d-xl-none" type="button">
	                    <span></span>
	                    <span></span>
	                    <span></span>
	                </button>
	                <div class="site-logo"><a class="navbar-brand" href="/index.html"><img class="logo-icon me-2" src="/assets/images/coderdocs-logo.svg" alt="logo"><span class="logo-text">Bhanu<span class="text-alt">.AI</span></span></a></div>    
                </div><!--//docs-logo-wrapper-->
	            <div class="docs-top-utilities d-flex justify-content-end align-items-center">
	                <div class="top-search-box d-none d-lg-flex">
		                <form class="search-form">
				            <input type="text" placeholder="Search the docs..." name="search" class="form-control search-input">
				            <button type="submit" class="btn search-btn" value="Search"><i class="fas fa-search"></i></button>
				        </form>
	                </div>
	
					<ul class="social-list list-inline mx-md-3 mx-lg-5 mb-0 d-none d-lg-flex">
						<li class="list-inline-item"><a href="https://github.com/bhanuprasanna527" target="_blank" rel="noopener noreferrer"><i class="fab fa-github fa-fw"></i></a></li>
			            <li class="list-inline-item"><a href="https://twitter.com/bhanu527" target="_blank" rel="noopener noreferrer"><i class="fab fa-twitter fa-fw"></i></a></li>
		                <li class="list-inline-item"><a href="https://www.linkedin.com/in/bhanu-prasanna-a1b71b1a2/" target="_blank" rel="noopener noreferrer"><i class="fab fa-linkedin fa-fw"></i></a></li>
		                <li class="list-inline-item"><a href="https://www.instagram.com/bhanuprasanna527/" target="_blank" rel="noopener noreferrer"><i class="fab fa-instagram fa-fw"></i></a></li>
		            </ul><!--//social-list-->
		            <a href="https://github.com/bhanuprasanna527/Bhanu_AI_Notes" class="btn btn-primary d-none d-lg-flex" target="_blank" rel="noopener noreferrer">GITHUB üåü</a>
	            </div><!--//docs-top-utilities-->
            </div><!--//container-->
        </div><!--//branding-->
    </header><!--//header-->
    
    
    <div class="docs-wrapper">
	    <div id="docs-sidebar" class="docs-sidebar">
		    <div class="top-search-box d-lg-none p-3">
                <form class="search-form">
		            <input type="text" placeholder="Search the docs..." name="search" class="form-control search-input">
		            <button type="submit" class="btn search-btn" value="Search"><i class="fas fa-search"></i></button>
		        </form>
            </div>
		    <nav id="docs-nav" class="docs-nav navbar">
			    <ul class="section-items list-unstyled nav flex-column pb-3">
				    <li class="nav-item section-title"><a class="nav-link scrollto active" href="#section-1"><span class="theme-icon-holder me-2"><i class="fa-solid fa-1"></i></span>Unsupervised Learning</a></li>
				    <li class="nav-item"><a class="nav-link scrollto" href="#item-1-1">Beyond Supervised Learning</a></li>
				    <li class="nav-item section-title mt-3"><a class="nav-link scrollto" href="#section-2"><span class="theme-icon-holder me-2"><i class="fa-solid fa-2"></i></span>Clustering</a></li>
				    <li class="nav-item"><a class="nav-link scrollto" href="#item-2-1">What is Clustering?</a></li>
				    <li class="nav-item"><a class="nav-link scrollto" href="#item-2-2">K-Means Intuition</a></li>
				    <li class="nav-item"><a class="nav-link scrollto" href="#item-2-3">K-Means Algorithm</a></li>
					<li class="nav-item"><a class="nav-link scrollto" href="#item-2-4">K-Means Optimization</a></li>
				    <li class="nav-item"><a class="nav-link scrollto" href="#item-2-5">Initializing K-means</a></li>
				    <li class="nav-item"><a class="nav-link scrollto" href="#item-2-6">Choosing the number of clusters</a></li>
				    <li class="nav-item section-title mt-3"><a class="nav-link scrollto" href="#section-3"><span class="theme-icon-holder me-2"><i class="fa-solid fa-3"></i></span>Anomaly Detection</a></li>
				    <li class="nav-item"><a class="nav-link scrollto" href="#item-3-1">Finding Unusual events</a></li>
				    <li class="nav-item"><a class="nav-link scrollto" href="#item-3-2">Gaussian (Normal) Distribution</a></li>
				    <li class="nav-item"><a class="nav-link scrollto" href="#item-3-3">Anomaly Detection Algorithm</a></li>
					<li class="nav-item"><a class="nav-link scrollto" href="#item-3-4">Developing and Evaluating an Anomaly Detection System</a></li>
				    <li class="nav-item"><a class="nav-link scrollto" href="#item-3-5">Anomaly Detection vs. Supervised Learning</a></li>
				    <li class="nav-item"><a class="nav-link scrollto" href="#item-3-6">Choosing what features to use</a></li>
				    <li class="nav-item section-title mt-3"><a class="nav-link scrollto" href="#section-4"><span class="theme-icon-holder me-2"><i class="fa-solid fa-4"></i></span>Conclusion</a></li>
				    <li class="nav-item"><a class="nav-link scrollto" href="#item-4-1">What we have covered?</a></li>
			    </ul>

		    </nav><!--//docs-nav-->
	    </div><!--//docs-sidebar-->
	    <div class="docs-content">
		    <div class="container">
			    <article class="docs-article" id="section-1">
				    <header class="docs-header">
					    <h1 class="docs-heading">Unsupervised Learning<span class="docs-time">Last updated: 2019-06-01</span></h1>
					    <section class="docs-intro">
						    <p>The last 2 course of the Machine Learning Specialization discussed mainly about the Supervised Learning and it's methods (Classification and Regression). The 3<sup>rd</sup> is mostly focussed on Unsupervised Learning, Recommender Systems, and Reinforcement Learning.</p>
						</section><!--//docs-intro-->
				    </header>


				    <section class="docs-section" id="item-1-1">
						<h2 class="section-heading">Beyond Supervised Learning</h2>
						<p>A Supervised Learning algorithm only works when both input and output is provided. Unsupervised learning only works with just the input data without provided the output values. This can be done using Unsupervised algorithms like Clustering and many more.</p>
						<img class="figure-img img-fluid shadow rounded" src="1632916005843.jpeg" alt="" title="Unsupervised vs Supervised Learning">
						</section><!--//section-->
			    </article>
			    
			    <article class="docs-article" id="section-2">
				    <header class="docs-header">
					    <h1 class="docs-heading">Clustering</h1>
					    <section class="docs-intro">
						    <p>Let us learn more about Clustering in this section.</p>
						</section><!--//docs-intro-->
				    </header>
					<section class="docs-section" id="item-2-1">
						<h2 class="section-heading">What is Clustering?</h2>
						<p>Clustering is a Unsupervised Learning algorithm which does not use labelled data but uses Unlabelled data and groups the data into individual clusters which makes this algorithm so much useful than Supervised Learning algorithms as we need to specify the output prehand in order for the Supervised Learning Algorithms to learn from the provided data.</p>
						<br><img class="figure-img img-fluid shadow rounded" src="https://miro.medium.com/max/1400/1*4VnrpMInuiPkHG0904Fblg.png" alt="" title="Clustering">
						<br><br>
						<h3>Applications of Unsupervised Learning</h3>
						<li>Grouping Similar News. (<b>Eg:</b> Group about Pandas,...;)</li>
						<li>DNA Analysis.</li>
						<li>Astronomical Data Analysis - Here Astronomers use Clustering algorithms to cluster some part of the space in order to analyze better.</li>
						<li>These are Applications of Unsupervised Learning and there are many areas where Unsupervised learning gives an advantage over Supervised Learning.</li>
						<p>Let us look and learn the first Clustering Algorithm which is the K-Means Clustering Algorithm.</p>
						</section><!--//section-->
					
					<section class="docs-section" id="item-2-2">
						<h2 class="section-heading">K-Means Intuition</h2>
						<p>K-Means is an Clustering Algorithm which is a Unsupervised Algorithm. In K-Means first plots all the data points and loops 2 operations in order to find different clusters present in the given data.</p>
						<p>The two steps are:</p>
						<ol>
							<li>Assign every point to the cluster centroid, depending on what cluster centroid is nearest to.</li>
							<li>Move each cluster centroid to the average or the mean of all the points that were assigned to it.</li>
						</ol>
						<br>
						<center><img class="figure-img img-fluid shadow rounded" src="WL1tIZ4.gif" alt="" title="K-Means Clustering Algorithm" width="60%" height="60%"></center>
						<br><br>
						<p>Now, let us discuss more about the inner working and learn about the K-Means algorithm.</p>
						</section><!--//section-->

						<section class="docs-section" id="item-2-3">
							<h2 class="section-heading">K-Means Algorithm</h2>
							<p>The K-means algorithm is a method to automatically cluster similar data points together.</p>
							<p>In the clustering problem, we are given a training set  ùë•<sup>(1)</sup>,...,ùë•<sup>(ùëö)</sup> , and want to group the data into a few cohesive "clusters." Here, we are given feature vectors for each data point  ùë•<sup>(ùëñ)</sup> ‚àà ‚Ñùùëõ  as usual; but no labels  ùë¶<sup>(ùëñ)</sup>  (making this an unsupervised learning problem). Our goal is to predict  ùëò  centroids and a label  ùëê<sup>(ùëñ)</sup>  for each datapoint. The k-means clustering algorithm is as follows:</p>
							<center><img class="figure-img img-fluid shadow rounded" src="kmeansMath.png" alt="" title="K-Means Algorithm"></center>
							<div class="docs-code-block">
								<pre class="shadow-lg rounded"><code class="language-python">
Randomly initialize K cluster centroid u<sub>1</sub>, u<sub>2</sub>, ...., u<sub>k</sub>.
Repeat {
	# Assign points to cluster centroids
	for i = 1 to m
		c<sup>(i)</sup> := index (from 1 to K) of cluster centroid closest to x<sup>(i)</sup>
	# Move cluster centroids
	for k = 1 to K
		u<sub>k</sub> := average (mean) of points assigned to cluster k
}
	</code></pre>
							</div><!--//docs-code-block-->

							<li>K-means is an iterative procedure that</li>
							<li style="margin-left: 3rem">Starts by guessing the initial centroids, and then</li>
							<li style="margin-left: 3rem">Refines this guess by</li>
							<li style="margin-left: 6rem">Repeatedly assigning examples to their closest centroids, and then</li>
							<li style="margin-left: 6rem">Recomputing the centroids based on the assignments.</li>
							<p><b>Let us understand the algorithm in depth:</b></p>
							<li>The inner-loop of the algorithm repeatedly carries out two steps:</li>
							<li style="margin-left: 3rem">Assigning each training example x<sup>(i)</sup> to its closest centroid, and</li>
							<li style="margin-left: 3rem">Recomputing the mean of each centroid using the points assigned to it.</li>
							<li>The  ùêæ -means algorithm will always converge to some final set of means for the centroids.</li>
							<li>However, that the converged solution may not always be ideal and depends on the initial setting of the centroids.</li>
							<li style="margin-left: 3rem">Therefore, in practice the K-means algorithm is usually run a few times with different random initializations.</li>
							<li style="margin-left: 3rem">One way to choose between these different solutions from different random initializations is to choose the one with the lowest cost function value (distortion).</li>

							<div class="docs-code-block">
								<pre class="shadow-lg rounded"><code class="language-python">
# Initialize centroids
# K is the number of clusters
centroids = kMeans_init_centroids(X, K)

for iter in range(iterations):
	# Cluster assignment step: 
	# Assign each data point to the closest centroid. 
	# idx[i] corresponds to the index of the centroid 
	# assigned to example i
	idx = find_closest_centroids(X, centroids)

	# Move centroid step: 
	# Compute means based on centroid assignments
	centroids = compute_means(X, idx, K)
	</code></pre>
							</div><!--//docs-code-block-->
						</section><!--//section-->

						<section class="docs-section" id="item-2-4">
							<h2 class="section-heading">K-Means Optimization</h2>
							<p>As we have seen in the previous courses that Gradient Descent was used as the optimization technique in order to reduce the cost of the mode. Here also we need a optimization technique but it is not gradient descent but it is called as <b>Distortion .</b></p>
							<p>The objective of K-Means clustering is to minimize total intra-cluster variance, or, the squared error function:</p>
							<p>Distortion Measure can be either Sum of the square of the distance of each example to its nearest cluster center or Average of the euclidean squared distance from the centroid of the respective clusters.</p>
							<p>$$J(c^{(1)}, c^{(2)}, ...., c^{(m)}, \mu_{1}, \mu_{2}, ...., \mu_{K}) = \frac{1}{m} \sum_{i=1}^{m} \left\| x^{(i)} - \mu_{c^{(i)}} \right\|^{2}$$</p>
							<p>where</p>
							<p>$$c^{(i)} = index\ of\ cluster\ (1,2, ..., K)\ to\ which\ example\ x^{(i)}\ is\ currently\ assigned$$</p>
							<p>$$\mu_{k} = cluster\ centroid\ k$$</p>
							<p>$$\mu_{c^{(i)}} = cluster\ centroid\ of\ cluster\ to\ which\ example\ x^{(i)}\ has\ been\ assigned$$</p>
							<p>This is the equation used for K-Means Optimization.</p>
						</section><!--//section-->

						<section class="docs-section" id="item-2-5">
							<h2 class="section-heading">Initializing K-means</h2>
							<p>Random intialize K cluster centroids and make sure that number of cluster centroids is always less than the number of training examples i.e, <code>K < m</code>.</p>
							<p>The K-means algorithm has a problem called the random initialization trap.When the centroids of the clusters that are to be generated are explicitly defined by the user in the random initialization trap, inconsistencies may be created, which may occasionally result in the dataset being generated with the wrong clusters.As a result, the random initialization trap may occasionally prevent us from creating the appropriate clusters.</p>
							<img class="figure-img img-fluid shadow rounded" src="https://i2.wp.com/content.edupristine.com/images/blogs/k-means-algorithm_4.png?zoom=2&w=525&ssl=1" alt="" title="Random Intialization Trap">
							<br><br>
							<p>k-Means clustering is prone to initial seeding i.e. random initialization of centroids which is required to kick-off iterative clustering process. Bad initialization may end up getting bad clusters. The above diagram shows one example where for if initial seed happens to be one each in red, blue and green regions then we get true clustering (left image), but we may get wrong clustering (middle, right images) at other initial location.</p>
							<div class="docs-code-block">
								<pre class="shadow-lg rounded"><code class="language-python">
for i = 1 to 100 {
	Randomly initialize K-Means (Pick K random examples and set as cluster centroids)
	Run K-Means. Get c<sup>(1)</sup>, ...., c<sup>(m)</sup> and Œº<sub>1</sub>, ...., Œº<sub>k</sub>
	Compute Cost Function (Distortion) (J(c<sup>(1)</sup>, ...., c<sup>(m)</sup> , Œº<sub>1</sub>, ...., Œº<sub>k</sub>))
}
Pick set of cluster that gave lowest J (Cost)
	</code></pre>
							</div><!--//docs-code-block-->
							<p>It is costly since multiple iterations must be run to find the appropriate clusters which has the lowest cost.</p>
						</section><!--//section-->

						<section class="docs-section" id="item-2-6">
							<h2 class="section-heading">Choosing the number of clusters</h2>
							<p>Choosing the number of clusters is always a ambiguous task. No one can truly tell if the choosen number of clusters is truly dependable.</p>
							<p>There have methods developed in academia like Elbow method to choose the number of clusters. For this we need to plot the graph between Number of clusters and Cost Function.</p>
							<img class="figure-img img-fluid shadow rounded" src="https://www.oreilly.com/api/v2/epubs/9781788295758/files/assets/995b8b58-06f1-4884-a2a1-f3648428e947.png" alt="" title="Elbow Point for choosing number of clusters">
							<br><br>
							<p>But normally the cost function does not always have a elbow present. It will mostly have a smooth curve which makes the elbow method not viable.</p>
							<p><b>Note: </b>Never choose K just to minimize the Cost Function J. This means that never choose the highest value of clusters for the sole purpose to reduce Cost.</p>
							<p>Let us see what is a good method to choose the number of clusters. The number of clusters choosen must be based on the problem that you are dealing with. Let us take the example of a T-shirt Business. They want to cluster their size profiles to manage their inventory.</p>
							<p>If the there are 3 sizes S, M, L then we might need 3 clusters. If we would group them for further future purpose also then let us take 5 sizes XS, S, M, L, XL then we might need 5 clusters. Let us take visual look at it:</p>
							<img class="figure-img img-fluid shadow rounded" src="Screenshot 2022-10-08 at 6.34.59 AM.png" alt="" title="Correct way of choosing number of clusters">
						</section><!--//section-->
			    </article><!--//docs-article-->
			    
			    
			    <article class="docs-article" id="section-3">
				    <header class="docs-header">
					    <h1 class="docs-heading">Anomaly Detection</h1>
					    <section class="docs-intro">
						    <p>Anomalies, often referred to as outliers, abnormalities, rare events, or deviants, are data points or patterns in data that do not conform to a notion of normal behavior. Anomaly detection, then, is the task of finding those patterns in data that do not adhere to expected norms, given previous observations. The capability to recognize or detect anomalous behavior can provide highly useful insights across industries. Flagging unusual cases or enacting a planned response when they occur can save businesses time, costs, and customers. Hence, anomaly detection has found diverse applications in a variety of domains, including IT analytics, network intrusion analytics, medical diagnostics, financial fraud protection, manufacturing quality control, marketing and social media analytics, and more.</p>
						</section><!--//docs-intro-->
				    </header>

				     <section class="docs-section" id="item-3-1">
						<h2 class="section-heading">Finding Unusual events</h2>
						<p>Let us look at the example of Aircraft Engine Manufacturing and how we can use Anomaly Detection in this aspect.</p>
						<img class="figure-img img-fluid shadow rounded" src="Screenshot 2022-10-08 at 7.35.42 AM.png" alt="" title="Aircraft Engine manufacturing">
						<br>
						<p>As we can see the Aircraft engine has features Heat generated and vibration intensity. So, if we take a dataset of m engines with these features then if we plot them we could see that visually in the above picture. We can see the region where the engines made can be classified as no anomaly but if exceeds a certain region we can see that there are not that many engines made present we can classify that as an Anomaly.</p>
						<h3>Density Estimation</h3>
						<p>Let us look at Density Estimation and how it plays a role in Anomaly Detection.</p>
						<p>In probability and statistics, density estimation is the construction of an estimate, based on observed data, of an unobservable underlying probability density function. Density estimation is estimating the probability density function of the population from the sample.</p>
						<p>Let us use the same problem of Aircraft Engine Manufacturing and find the engine that has defects.</p>
						<p>Density Estimation creates regions using probability of that to occur. As we can see in the image below, there are 3 regions created where the center region has the highest probability of the engines to be made without any defect. As we go outwards of the center region we can see that the probability decreases and there is likely a chance for the engine to show defects.</p>
						<p>If we take a new engine for test and if we plot the data generated by the engine we can see the region it falls and we can check it with a variable <code>Œµ</code>. We can use the variable and check for probability <code>p(x<sub>test</sub>) < Œµ</code>. Where Œµ is a small number. Using this we can check for Anomaly Detection. Let us discuss more about the Probability function (Gaussian Distribution) in the next section.</p>
						<img class="figure-img img-fluid shadow rounded" src="Screenshot 2022-10-08 at 7.43.50 AM.png" alt="" title="Density Estimation">
						<h3>Anomaly Detection Examples</h3>
						<p>Let us discuss some examples of Anomaly Detection:</p>
						<li><b>Fraud Detection</b></li>
						<li style="margin-left: 3rem;">x<sup>(i)</sup> = features of user ùëñ‚Äôs activities</li>
						<li style="margin-left: 3rem;">Model ùëù(ùë•) from data.</li>
						<li style="margin-left: 3rem;">Identify unusual users by checking which have ùëù(ùë•) < ùúÄ</li>
						<li><b>Monitoring computers in a data center:</b><br>&emsp;  ùë•<sup>(i)</sup>= features of machine ùëñ</li>
						<li style="margin-left: 3rem;">ùë•<sub>1</sub> = memory use</li>
						<li style="margin-left: 3rem;">ùë•<sub>2</sub> = number of disk accesses/sec</li>
						<li style="margin-left: 3rem;">ùë•<sub>3</sub> = CPU load</li>
						<li style="margin-left: 3rem;">ùë•<sub>4</sub> = CPU load/network traffic</li>
						<li>Some other examples would be Network Intrusion, Manufacturing and many more.</li>

					</section><!--//section-->
					
					<section class="docs-section" id="item-3-2">
						<h2 class="section-heading">Gaussian (Normal) Distribution</h2>
						<p>Gaussian Distribution is also called as Normal Distribution or Bell-shaped Distribution.</p>
						<li>Say x is a number. Then, Probability of ùë• denoted by P(ùë•) is determined by a Gaussian with mean ùúá, variance ùúé<sup>2</sup>.</li>
						<p>Let us see the equation used for this:</p>
						<p>$$p(x) = \frac{1}{\sqrt{2\pi} \sigma} e^{\frac{-(x-\mu)^{2}}{2\sigma^{2}}}$$</p>
						<p>$$Mean = \mu = \frac{1}{m} \sum_{i=1}^{m} x^{(i)}$$</p>
						<p>$$Variance = \sigma^{2} = \frac{1}{m} \sum_{i=1}^{m} (x^{(i)} - \mu)^{2}$$</p>
						<img class="figure-img img-fluid shadow rounded" src="https://miro.medium.com/max/800/1*w5HzgB5ekxQ6Nwmx5ggn8Q.jpeg" alt="" title="Gaussian Distribution">
						<br>
						<p>Now, we have seen <b>Parameter Estimation</b> using 1 features. But there will be many features present. So, let us discuss how to tackle this in the next section.</p>
					</section><!--//section-->
					
					<section class="docs-section" id="item-3-3">
						<h2 class="section-heading">Anomaly Detection Algorithm</h2>
						<p>In the previous section, we have seen Parameter Estimation i.e, using Gaussian Distribution for 1 feature. This section we will see how to implement for many features and also look into the Anomaly Detection Algorithm.</p>
						<p>Let us take the previous example, Aircraft Engine Manufacturing where there were 2 features Temperature and Vibration.</p>
						<p>$$Training\ set:\ {\vec{x}^{(1)}, \vec{x}^{(2)}, ...., \vec{x}^{(m)}}$$</p>
						<p>$$Each\ example\ \vec{x}^{(i)}\ has\ n\ features$$</p>
						<p>where, $$\vec{x} = \begin{bmatrix}
							x_{1} \\
							x_{2} \\
							\vdots \\
							x_{n}
						  \end{bmatrix}$$</p>
						  <p>$$p(\vec{x}) = p(x_{1}; \mu_{1}, \sigma_{1}^{2}) *\ p(x_{2}; \mu_{2}, \sigma_{2}^{2}) *\ .... *\ p(x_{n}; \mu_{n}, \sigma_{n}^{2})$$</p>
						  <p>Short form for the above equation is:</p>
						  <p>$$p(\vec{x}) = \prod_{j=1}^{n} p(x_{j}; \mu_{j}, \sigma_{j}^{2})$$</p>
						  <h3>Algorithm</h3>
						  <p>As we have seen the equations for Density Estimation i.e, to implement for more features. Let us see the algorithm for Anomaly Detection:</p>
						  <ol>
							<li>Choose ùëõ features ùë•<sub>i</sub> that you think might be indicative of anomalous examples.</li>
							<li>Fit parameters Œº<sub>1</sub>, ...., Œº<sub>n</sub>, œÉ<sub>1</sub><sup>2</sup>, ...., œÉ<sub>n</sub><sup>2</sup>.</li>
							<p>$$\mu_{j} = \frac{1}{m} \sum_{i=1}^{m} x_{j}^{(i)}$$</p>
							<p>$$\sigma_{j}^{2} = \frac{1}{m} \sum_{i=1}^{m} (x_{j}^{(i)} - \mu_{j})^{2}$$</p>
							<li>Given new example ùë•, compute p(ùë•):</li>
							<p>$$p(x) = \prod_{j=1}^{n} p(x_{j}; \mu_{j}, \sigma_{j}^{2}) = \prod_{j=1}^{n} \frac{1}{\sqrt{2\pi} \sigma} e^{\frac{-(x-\mu)^{2}}{2\sigma^{2}}}$$</p>
							<li><b>Final Step: </b>Anomaly if ùëù(ùë•) < ùúÄ</li>
						  </ol>
						  <li><b>This is the Algorithm for Anomaly Detection.</b></li>
					</section><!--//section-->

					<section class="docs-section" id="item-3-4">
						<h2 class="section-heading">Developing and Evaluating an Anomaly Detection System</h2>
						<p>In the last section we have discussed about the algorithm for Anomaly Detection. If you have created a model it is better to keep some test cases for both anomaly cases and as well as non-anomaly cases for evaluating the Anomaly detection model.</p>
						<p>When developing a learning algorithm (choosing features, etc.), making decisions is much easier if we have a way of evaluating our learning algorithm.</p>
						<p>Assume we have some labeled data, of anomalous and non-anomalous examples.</p>

						<p>As we have seen in previous course the evaluating techniques for a model:</p>
						<li>Using Train and Test set does not produce that much of an impact. So, we should divide the dataset into Train, Cross-validation (Validation or Dev set) and Test Set.</li>

						<p>Let us take the previous example Aircraft engines monitoring example:</p>
						<p>Let us take a dataset containing 10,000 good (normal) engines and 20 flawed engines.</p>
						<p>Let us divide into 2 cases:</p>
						<h4>CASE 1:</h4>
						<p>Here, we will have a Training set only containing good engines. Let it be 6000 samples.</p>
						<p>Let the Cross-validation set contain 2000 good and 10 flawed engines.</p>
						<p>Let the Test set contain 2000 good and 10 flawed engines.</p>
						<h4>CASE 2:</h4>
						<p>Here, we will have only 2 sets of evaluation: Train set and Cross-validation set for evaluation.</p>
						<p>Let the Train set contain only good engines. Let it be 6000 samples.</p>
						<p>In the Cross-validation set let the good engines be 4000 and the flawed engines be 20.</p>

						<p><b>Let us discuss in detail about the cases above:</b></p>
						<li>In case 2 as there is no test set there is higher risk of overfitting. Case 2 must be only used when we have very few labeled anomalous examples for evaluation.</li>
						<h3>Algorithm Evaluation</h3>
						<p>Let us go through the steps once more:</p>
						<li>Fit model p(x) on training set x<sup>(1)</sup>, x<sup>(2)</sup>, ... , x<sup>(m)</sup></li>
						<li>On a cross validation/test example x, predict</li>
						<p>$$y\ = 
							\begin{cases}
								1,& \text{if } p(x) < Œµ\ (anomaly)\\
								0,              & \text{if } p(x) \geq Œµ (normal) 
							\end{cases}$$</p>

						<p>In the last course we have seen some possible evaluation metrics:</p>
						<li>True positive, false positive, false negative, true negative</li>
						<li>Precision/ Recall</li>
						<li>F1-score</li>
						<p>The <b>most effective method of evaluation </b> is to use Cross-Validation set to choose parameter Œµ.</p>
					</section><!--//section-->

					<section class="docs-section" id="item-3-5">
						<h2 class="section-heading">Anomaly Detection vs. Supervised Learning</h2>
						<div class="table-responsive my-4">
							<table class="table table-bordered table-dark">
								<thead>
									<tr>
										<th scope="col" style="font-size: large">Anomaly Detection</th>
										<th scope="col" style="font-size: large">Supervised Learning</th>
									</tr>
								</thead>
								<tbody>
									<tr>
										<td>Very small number (0 to 20) of
											positive examples ùë¶ = 1.</td>
										<td>Large number of positive examples.</td>
									</tr>
									<tr>
										<td>Large number of negative examples (ùë¶ = 0).</td>
										<td>Large number of negative examples.</td>
									</tr>
									<tr>
										<td>Train Model ùëù(ùë•) with just negative examples. Use positive examples for cv and test sets.</td>
										<td>Train, Validate, and Test Model with both positive and negative examples.</td>
									</tr>
									<tr>
										<td>Many different ‚Äútypes‚Äù of anomalies. Hard for any algorithm to learn (from just positive examples) what the anomalies look like.</td>
										<td>Enough positive examples for algorithm to get a sense of what positive examples are like.</td>
									</tr>
									<tr>
										<td>Future anomalies may look nothing like any of the anomalous examples seen so far.</td>
										<td>Future positive examples likely to be similar to ones in training set.</td>
									</tr>
									<tr>
										<td style="font-size: large"><b>Examples of Anomaly Detection</b></td>
										<td style="font-size: large"><b>Examples of Supervised Learning</b></td>
									</tr>
									<tr>
										<td>
											<ol>
												<li>Financial Fraud Detection</li>
												<li>Manufacturing:</li>
												<p>Finding new previously unseen defects. (e.g. Aircraft Engines)</p>
												<li>Monitoring machines in a data center</li>
												<li>Security applications</li>
											</ol>
										</td>
										<td>
											<ol>
												<li>Email Spam Classification</li>
												<li>Manufacturing:</li>
												<p>Finding known, previously seen defects (e.g. scratches on smartphones, y = 1)</p>
												<li>Weather prediction (sunny/rainy/etc.)</li>
												<li>Diseases classification</li>
											</ol>
										</td>
									</tr>
								</tbody>
							</table>
						</div><!--//table-responsive-->
					</section><!--//section-->

					<section class="docs-section" id="item-3-6">
						<h2 class="section-heading">Choosing what features to use</h2>
						<p>Let us discuss about Non-gaussian features. Anomaly detection that we have seen is mainly derived from Gaussian Distribution or Normal Distribution. So, the shape of the distribution must be in a bell-shaped.</p>
						<p>In supervised learning, if you don't have the features quite right, or if you have a few extra features that are not relevant to the problem, that often turns out to be okay. Because the algorithm has to supervised signal that is enough labels why for the algorithm to figure out what features ignore, or how to re scale feature and to take the best advantage of the features you do give it. But for anomaly detection which runs, or learns just from unlabeled data, is harder for the anomaly to figure out what features to ignore. So I found that carefully choosing the features, is even more important for anomaly detection, than for supervised learning approaches.</p>
						<li>One step that can help your anomaly detection algorithm, is to try to make sure the features you give it are more or less Gaussian.</li>
						<img class="figure-img img-fluid shadow rounded" src="b2.png" alt="" title="Non-Gaussian to Gaussian Features">
						<br>
						<p>Other than making sure that your data is approximately Gaussian, after you've trained your anomaly detection algorithm, if it doesn't work that well on your trust validation set, you can also carry out an error analysis process for anomaly detection.</p>
						<h3>Error analysis for Anomaly Detection</h3>
						<p>Let us take example of Fraud Detection. Here let us take one feature Number of transactions. If we plot the Gaussian Distribution then test the model with test examples which are anomalies. If it predicts wrong then we can add one more feature like typing speed in order to increase the chance of getting to know the anomalies present.</p>
						<img class="figure-img img-fluid shadow rounded" src="Screenshot 2022-10-08 at 12.48.32 PM.png" alt="" title="Choosing features for analysis">
						<br><p>So just to summarize about the development process will often go through is, to train the model and then to see what anomalies, in the cross validation set the algorithm is failing to detect. And then to look at those examples to see if that can inspire the creation of new features that would allow the algorithm to spot. That example takes on unusually large or unusually small values on the new features, so that you can now successfully flag those examples as anomalies.</p>
						<h3>Monitoring computer in a data center</h3>
						<p>Let us take another example for anomoly detection:</p>
						<p>Choose features that might take on unusually large or small values in the event of an anomaly.</p>
						<p>Features in this example are:</p>
						<li>x<sub>1</sub> = memory use of computer</li>
						<li>x<sub>2</sub> = number of disk accesses/sec</li>
						<li>x<sub>3</sub> = CPU load</li>
						<li>x<sub>4</sub> = network traffic</li>
						<p>We can use the above features to create more features or use the concept Feature Engineering discussed in earlier courses of this specialization.</p>
						<li>x<sub>5</sub> = CPU load / network traffic</li>
						<li>x<sub>6</sub> = (CPU load)<sup>2</sup> / network traffic</li>
						<p>This type of increasing the number of features can help the model to effectively predict the anomalies presented by new samples or examples.</p>
						<p>Deciding feature choice based on ùëù(ùë•)</p>
						<p>&emsp;Large for normal examples.</p>
						<p>&emsp;Becomes small for anomaly in the cross validation set.</p>
					</section><!--//section-->

			    </article><!--//docs-article-->
			    
			    <article class="docs-article" id="section-4">
				    <header class="docs-header">
					    <h1 class="docs-heading">Conclusion</h1>
					    <section class="docs-intro">
						    <p>This is the end of Week 1 of Machine Learning Specialization Course 3. Let's enter Week 2.</p>
						</section><!--//docs-intro-->
				    </header>
				     <section class="docs-section" id="item-4-1">
						<h2 class="section-heading">What we have covered?</h2>
						<li>Learnt about Clustering and Anomaly Detection</li>
						<li>Learnt and implemented K-Means Clustering Algorithm</li>
						<li>Learnt about Gaussian Distribution</li>
						<li>Decide when to choose Anomaly Detection vs Supervised Learning</li>
						<li>Learnt about Choosing the number of Clusters</li>
						<li>Algorithm of Anomaly Detection</li>
					</section><!--//section-->
			    </article><!--//docs-article-->


			    <footer class="footer">
				    <div class="container text-center py-5">
				        <!--/* This template is free as long as you keep the footer attribution link. If you'd like to use the template without the attribution link, you can buy the commercial license via our website: themes.3rdwavemedia.com Thank you for your support. :) */-->
			            <!-- <small class="copyright">Designed with <span class="sr-only">love</span><i class="fas fa-heart" style="color: #fb866a;"></i> by <a class="theme-link" href="http://themes.3rdwavemedia.com" target="_blank">Xiaoying Riley</a> for developers</small> -->
				        <ul class="social-list list-unstyled pt-4 mb-0">
						    <li class="list-inline-item"><a href="https://github.com/bhanuprasanna527" target="_blank" rel="noopener noreferrer"><i class="fab fa-github fa-fw"></i></a></li>
			            <li class="list-inline-item"><a href="https://twitter.com/bhanu527" target="_blank" rel="noopener noreferrer"><i class="fab fa-twitter fa-fw"></i></a></li>
		                <li class="list-inline-item"><a href="https://www.linkedin.com/in/bhanu-prasanna-a1b71b1a2/" target="_blank" rel="noopener noreferrer"><i class="fab fa-linkedin fa-fw"></i></a></li>
		                <li class="list-inline-item"><a href="https://www.instagram.com/bhanuprasanna527/" target="_blank" rel="noopener noreferrer"><i class="fab fa-instagram fa-fw"></i></a></li>
				        </ul><!--//social-list-->
				    </div>
			    </footer>
		    </div> 
	    </div>
    </div><!--//docs-wrapper-->
   
       
    <!-- Javascript -->          
    <script src="/assets/plugins/popper.min.js"></script>
    <script src="/assets/plugins/bootstrap/js/bootstrap.min.js"></script>  
    
    
    <!-- Page Specific JS -->
    <script src="/assets/plugins/smoothscroll.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.8/highlight.min.js"></script>
    <script src="/assets/js/highlight-custom.js"></script> 
    <script src="/assets/plugins/simplelightbox/simple-lightbox.min.js"></script>      
    <script src="/assets/plugins/gumshoe/gumshoe.polyfills.min.js"></script> 
    <script src="/assets/js/docs.js"></script> 

</body>
</html> 

