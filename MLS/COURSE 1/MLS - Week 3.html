<!DOCTYPE html>
<html lang="en"> 
<head>
    <title>Bhanu.AI Machine Learning Specialization</title>
    
    <!-- Meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="AI Notes written by Bhanu Prasanna from Various Specializations.">
    <meta name="author" content="Bhanu prasanna">    
    <link rel="shortcut icon" href="/favicon.ico">
    

	<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Google Font -->
    <link href="https://fonts.googleapis.com/css?family=Poppins:300,400,500,600,700&display=swap" rel="stylesheet">
    
    <!-- FontAwesome JS-->
    <script defer src="/assets/fontawesome/js/all.min.js"></script>
    
    <!-- Plugins CSS -->
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.2/styles/atom-one-dark.min.css">
    <link rel="stylesheet" href="/assets/plugins/simplelightbox/simple-lightbox.min.css">

    <!-- Theme CSS -->  
    <link id="theme-style" rel="stylesheet" href="/assets/css/theme.css">

</head> 

<body class="docs-page">    
    <header class="fixed-top">	    
        <div class="branding docs-branding">
            <div class="container-fluid position-relative py-2">
                <div class="docs-logo-wrapper">
					<button id="docs-sidebar-toggler" class="docs-sidebar-toggler docs-sidebar-visible me-2 d-xl-none" type="button">
	                    <span></span>
	                    <span></span>
	                    <span></span>
	                </button>
	                <div class="site-logo"><a class="navbar-brand" href="/index.html"><img class="logo-icon me-2" src="/assets/images/coderdocs-logo.svg" alt="logo"><span class="logo-text">Bhanu<span class="text-alt">.AI</span></span></a></div>    
                </div><!--//docs-logo-wrapper-->
	            <div class="docs-top-utilities d-flex justify-content-end align-items-center">
	                <div class="top-search-box d-none d-lg-flex">
		                <form class="search-form">
				            <input type="text" placeholder="Search the docs..." name="search" class="form-control search-input">
				            <button type="submit" class="btn search-btn" value="Search"><i class="fas fa-search"></i></button>
				        </form>
	                </div>
	
					<ul class="social-list list-inline mx-md-3 mx-lg-5 mb-0 d-none d-lg-flex">
						<li class="list-inline-item"><a href="https://github.com/bhanuprasanna527" target="_blank" rel="noopener noreferrer"><i class="fab fa-github fa-fw"></i></a></li>
			            <li class="list-inline-item"><a href="https://twitter.com/bhanu527" target="_blank" rel="noopener noreferrer"><i class="fab fa-twitter fa-fw"></i></a></li>
		                <li class="list-inline-item"><a href="https://www.linkedin.com/in/bhanu-prasanna-a1b71b1a2/" target="_blank" rel="noopener noreferrer"><i class="fab fa-linkedin fa-fw"></i></a></li>
		                <li class="list-inline-item"><a href="https://www.instagram.com/bhanuprasanna527/" target="_blank" rel="noopener noreferrer"><i class="fab fa-instagram fa-fw"></i></a></li>
		            </ul><!--//social-list-->
		            <a href="https://github.com/bhanuprasanna527/Bhanu_AI_Notes" class="btn btn-primary d-none d-lg-flex" target="_blank" rel="noopener noreferrer">GITHUB üåü</a>
	            </div><!--//docs-top-utilities-->
            </div><!--//container-->
        </div><!--//branding-->
    </header><!--//header-->
    
    
    <div class="docs-wrapper">
	    <div id="docs-sidebar" class="docs-sidebar">
		    <div class="top-search-box d-lg-none p-3">
                <form class="search-form">
		            <input type="text" placeholder="Search the docs..." name="search" class="form-control search-input">
		            <button type="submit" class="btn search-btn" value="Search"><i class="fas fa-search"></i></button>
		        </form>
            </div>
		    <nav id="docs-nav" class="docs-nav navbar">
			    <ul class="section-items list-unstyled nav flex-column pb-3">
				    <li class="nav-item section-title"><a class="nav-link scrollto active" href="#section-1"><span class="theme-icon-holder me-2"><i class="fa-solid fa-1"></i></span>Logistic Regression</a></li>
				    <li class="nav-item"><a class="nav-link scrollto" href="#item-1-1">Motivation for Logistic Regression</a></li>
				    <li class="nav-item"><a class="nav-link scrollto" href="#item-1-2">Decision Boundary</a></li>
				    <li class="nav-item section-title mt-3"><a class="nav-link scrollto" href="#section-2"><span class="theme-icon-holder me-2"><i class="fas fa-arrow-down"></i></span>Cost Function for Logistic Regression</a></li>
				    <li class="nav-item"><a class="nav-link scrollto" href="#item-2-1">Logistic Loss Function</a></li>
				    <li class="nav-item"><a class="nav-link scrollto" href="#item-2-2">Gradient Descent</a></li>
				    <li class="nav-item section-title mt-3"><a class="nav-link scrollto" href="#section-3"><span class="theme-icon-holder me-2"><i class="fas fa-box"></i></span>Overfitting & Underfitting</a></li>
				    <li class="nav-item"><a class="nav-link scrollto" href="#item-3-1">Orginal Model</a></li>
				    <li class="nav-item"><a class="nav-link scrollto" href="#item-3-2">Overfitting - (High Variance)</a></li>
				    <li class="nav-item"><a class="nav-link scrollto" href="#item-3-3">Underfitting - (High Bias)</a></li>
					<li class="nav-item"><a class="nav-link scrollto" href="#item-3-4">Correct Fit - Bias-variance trade-off</a></li>
				    <li class="nav-item section-title mt-3"><a class="nav-link scrollto" href="#section-4"><span class="theme-icon-holder me-2"><i class="fas fa-cogs"></i></span>Regularization</a></li>
				    <li class="nav-item"><a class="nav-link scrollto" href="#item-4-1">Regularization in Logistic and Linear</a></li>
				    <li class="nav-item"><a class="nav-link scrollto" href="#item-4-2">Cost Function with Regularization</a></li>
				    <li class="nav-item"><a class="nav-link scrollto" href="#item-4-3">Gradient Descent with Regularization</a></li>
				    <li class="nav-item section-title mt-3"><a class="nav-link scrollto" href="#section-5"><span class="theme-icon-holder me-2"><i class="fas fa-tools"></i></span>Logistic Regression using Scikit-Learn</a></li>
				    <li class="nav-item"><a class="nav-link scrollto" href="#item-5-1">Model</a></li>
				    <li class="nav-item section-title mt-3"><a class="nav-link scrollto" href="#section-6"><span class="theme-icon-holder me-2"><i class="fas fa-laptop-code"></i></span>Conclusion</a></li>
				    <li class="nav-item"><a class="nav-link scrollto" href="#item-6-1">What we have covered?</a></li>
			    </ul>

		    </nav><!--//docs-nav-->
	    </div><!--//docs-sidebar-->
	    <div class="docs-content">
		    <div class="container">
			    <article class="docs-article" id="section-1">
				    <header class="docs-header">
					    <h1 class="docs-heading">Logistic Regression<span class="docs-time">Last updated: 2019-06-01</span></h1>
					    <section class="docs-intro">
						    <p>Most people confuse Logistic Regression is a regression algorithm but it is a classification algorithm. Logistic Regression makes use of sigmoid function. Let's dive deep and understand Cost function and Gradient descent for Logistic Regression. And finally we will discuss the problem of overfitting.</p>
						</section><!--//docs-intro-->
				    </header>

				    <section class="docs-section" id="item-1-1">
						<h2 class="section-heading">Motivation for Logistic Regression</h2>
                        <li>Linear regression is great algorithm, no doubt in that, but it can be used for identifying the relation between dependent and independent variable. So, it is used for basic applications.</li>
						<li>The major limitation is that Linear Regression Assumption of linearity between independent and dependent variables. In real world, the data is rarely Linear seperable. It assumes that there is a straight-line relationship between the dependent and independent variables which is incorrect many times.</li>
						<li>However, we would like the predictions of our classification model to be between 0 and 1 since our output variable  ùë¶  is either 0 or 1.</li>
							<li>This can be accomplished by using a "sigmoid function" which maps all input values to values between 0 and 1.</li>
						<li>The Logistic regression model uses Sigmoid function:</li>
						<p>$$g(z) = \frac{1}{1+e^{-z}}$$</p>
					</section><!--//section-->
					
					<section class="docs-section" id="item-1-2">
						<h2 class="section-heading">Decision Boundary</h2>
						<p>The fundamental application of logistic regression is to determine a decision boundary for a binary classification problem. Although the baseline is to identify a binary decision boundary, the approach can be very well applied for scenarios with multiple classification classes or multi-class classification.</p>

						<p>The dashed line is the decision boundary. The decision boundary can be a linear line or in any other format.</p>
						<p>The main activity performed by decision boundary is to separate so that we can predict which class it pertains to.</p>
						<p>The given below image provides a better representation of Decision boundary.</p>

						<img class="figure-img img-fluid shadow rounded" src="https://miro.medium.com/max/1400/1*aE8XLyApqvaQA9B7MWjjlA.png" alt="" title="Decision Boundary">
						
						
						
					</section><!--//section-->
			    </article>
			    
			    <article class="docs-article" id="section-2">
				    <header class="docs-header">
					    <h1 class="docs-heading">Cost Function for Logistic Regression</h1>
					    <section class="docs-intro">
						    <p>As we have seen in Linear Regression that the cost function is Squared Error Cost where the equation for one variable is:</p>
							<p>$$J(w,b) = \frac{1}{2m} \sum\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2$$</p>
							<p>where</p>
							<p>$$f_{w,b}(x^{(i)}) = wx^{(i)} + b$$</p>

							<p>For Linear Regression we get a Convex Cost graph, whereas for logistic regression we get a non-convex graph.</p>
							<img class="figure-img img-fluid shadow rounded" src="/assets/images/C1_W3_SqErrorVsLogistic.png" alt="" title="Linear Regression Squared Error vs Logistic Regression">
						</section><!--//docs-intro-->
				    </header>
				     <section class="docs-section" id="item-2-1">
						<h2 class="section-heading">Logistic Loss Function</h2>
						<p>Logistic Regression uses a loss function more suited to the task of categorization where the target is 0 or 1 rather than any number.</p>
						<p>Some basic definitions:</p>
						<li><b>Loss</b> is a measure of the difference of a single example to its target value while the</li>
						<li><b>Cost</b> is a measure of the losses over the training set</li>

						<p><math xmlns="http://www.w3.org/1998/Math/MathML">
							<semantics>
							  <mrow>
								<mi>l</mi>
								<mi>o</mi>
								<mi>s</mi>
								<mi>s</mi>
								<mo stretchy="false">(</mo>
								<msub>
								  <mi>f</mi>
								  <mrow class="MJX-TeXAtom-ORD">
									<mrow class="MJX-TeXAtom-ORD">
									  <mi mathvariant="bold">w</mi>
									</mrow>
									<mo>,</mo>
									<mi>b</mi>
								  </mrow>
								</msub>
								<mo stretchy="false">(</mo>
								<msup>
								  <mrow class="MJX-TeXAtom-ORD">
									<mi mathvariant="bold">x</mi>
								  </mrow>
								  <mrow class="MJX-TeXAtom-ORD">
									<mo stretchy="false">(</mo>
									<mi>i</mi>
									<mo stretchy="false">)</mo>
								  </mrow>
								</msup>
								<mo stretchy="false">)</mo>
								<mo>,</mo>
								<msup>
								  <mi>y</mi>
								  <mrow class="MJX-TeXAtom-ORD">
									<mo stretchy="false">(</mo>
									<mi>i</mi>
									<mo stretchy="false">)</mo>
								  </mrow>
								</msup>
								<mo stretchy="false">)</mo>
							  </mrow>
							  <annotation encoding="application/x-tex">loss(f_{\mathbf{w},b}(\mathbf{x}^{(i)}), y^{(i)})</annotation>
							</semantics>
						  </math> is the cost for a single data point, which is:
						  </p>
						  <p>\begin{equation}
							loss(f_{\mathbf{w},b}(\mathbf{x}^{(i)}), y^{(i)}) = \begin{cases}
							  - \log\left(f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right) & \text{if $y^{(i)}=1$}\\
							  - \log \left( 1 - f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right) & \text{if $y^{(i)}=0$}
							\end{cases}
						  \end{equation}
						  </p>

						  <li><math xmlns="http://www.w3.org/1998/Math/MathML">
							<semantics>
							  <mrow>
								<msub>
								  <mi>f</mi>
								  <mrow class="MJX-TeXAtom-ORD">
									<mrow class="MJX-TeXAtom-ORD">
									  <mi mathvariant="bold">w</mi>
									</mrow>
									<mo>,</mo>
									<mi>b</mi>
								  </mrow>
								</msub>
								<mo stretchy="false">(</mo>
								<msup>
								  <mrow class="MJX-TeXAtom-ORD">
									<mi mathvariant="bold">x</mi>
								  </mrow>
								  <mrow class="MJX-TeXAtom-ORD">
									<mo stretchy="false">(</mo>
									<mi>i</mi>
									<mo stretchy="false">)</mo>
								  </mrow>
								</msup>
								<mo stretchy="false">)</mo>
							  </mrow>
							  <annotation encoding="application/x-tex">f_{\mathbf{w},b}(\mathbf{x}^{(i)})</annotation>
							</semantics>
						  </math> is the model's prediction, while  <math xmlns="http://www.w3.org/1998/Math/MathML">
							<semantics>
							  <msup>
								<mi>y</mi>
								<mrow class="MJX-TeXAtom-ORD">
								  <mo stretchy="false">(</mo>
								  <mi>i</mi>
								  <mo stretchy="false">)</mo>
								</mrow>
							  </msup>
							  <annotation encoding="application/x-tex">y^{(i)}</annotation>
							</semantics>
						  </math>
							is the target value.
						  </li>
						  <li><math xmlns="http://www.w3.org/1998/Math/MathML">
							<semantics>
							  <mrow>
								<msub>
								  <mi>f</mi>
								  <mrow class="MJX-TeXAtom-ORD">
									<mrow class="MJX-TeXAtom-ORD">
									  <mi mathvariant="bold">w</mi>
									</mrow>
									<mo>,</mo>
									<mi>b</mi>
								  </mrow>
								</msub>
								<mo stretchy="false">(</mo>
								<msup>
								  <mrow class="MJX-TeXAtom-ORD">
									<mi mathvariant="bold">x</mi>
								  </mrow>
								  <mrow class="MJX-TeXAtom-ORD">
									<mo stretchy="false">(</mo>
									<mi>i</mi>
									<mo stretchy="false">)</mo>
								  </mrow>
								</msup>
								<mo stretchy="false">)</mo>
								<mo>=</mo>
								<mi>g</mi>
								<mo stretchy="false">(</mo>
								<mrow class="MJX-TeXAtom-ORD">
								  <mi mathvariant="bold">w</mi>
								</mrow>
								<mo>&#x22C5;<!-- ‚ãÖ --></mo>
								<msup>
								  <mrow class="MJX-TeXAtom-ORD">
									<mi mathvariant="bold">x</mi>
								  </mrow>
								  <mrow class="MJX-TeXAtom-ORD">
									<mo stretchy="false">(</mo>
									<mi>i</mi>
									<mo stretchy="false">)</mo>
								  </mrow>
								</msup>
								<mo>+</mo>
								<mi>b</mi>
								<mo stretchy="false">)</mo>
							  </mrow>
							  <annotation encoding="application/x-tex">f_{\mathbf{w},b}(\mathbf{x}^{(i)}) = g(\mathbf{w} \cdot\mathbf{x}^{(i)}+b)</annotation>
							</semantics>
						  </math> where function  ùëî  is the sigmoid function.
						  </li>

						  <p>The loss function can be rewritten to be easier to implement:</p>
						  <p>$$loss(f_{\mathbf{w},b}(\mathbf{x}^{(i)}), y^{(i)}) = (-y^{(i)} \log\left(f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right) - \left( 1 - y^{(i)}\right) \log \left( 1 - f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right)$$</p>
						  <p>This is a rather formidable-looking equation. It is less daunting when you consider<math xmlns="http://www.w3.org/1998/Math/MathML">
							<semantics>
							  <msup>
								<mi>y</mi>
								<mrow class="MJX-TeXAtom-ORD">
								  <mo stretchy="false">(</mo>
								  <mi>i</mi>
								  <mo stretchy="false">)</mo>
								</mrow>
							  </msup>
							  <annotation encoding="application/x-tex">y^{(i)}</annotation>
							</semantics>
						  </math>
						  can have only two values, 0 and 1. One can then consider the equation in two pieces:  
						</p>
						<p>When <math xmlns="http://www.w3.org/1998/Math/MathML">
							<semantics>
							  <msup>
								<mi>y</mi>
								<mrow class="MJX-TeXAtom-ORD">
								  <mo stretchy="false">(</mo>
								  <mi>i</mi>
								  <mo stretchy="false">)</mo>
								</mrow>
							  </msup>
							  <annotation encoding="application/x-tex">y^{(i)}</annotation>
							</semantics>
						  </math>
						  = 0 , the left-hand term is eliminated:</p>
						  <p>$$
							\begin{align}
							loss(f_{\mathbf{w},b}(\mathbf{x}^{(i)}), 0) &= (-(0) \log\left(f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right) - \left( 1 - 0\right) \log \left( 1 - f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right) \\
							&= -\log \left( 1 - f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right)
							\end{align}
							$$</p>
							<p>And when <math xmlns="http://www.w3.org/1998/Math/MathML">
								<semantics>
								  <msup>
									<mi>y</mi>
									<mrow class="MJX-TeXAtom-ORD">
									  <mo stretchy="false">(</mo>
									  <mi>i</mi>
									  <mo stretchy="false">)</mo>
									</mrow>
								  </msup>
								  <annotation encoding="application/x-tex">y^{(i)}</annotation>
								</semantics>
							  </math> = 1 , the right-hand term is eliminated:</p>
							  <p>$$
								\begin{align}
								  loss(f_{\mathbf{w},b}(\mathbf{x}^{(i)}), 1) &=  (-(1) \log\left(f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right) - \left( 1 - 1\right) \log \left( 1 - f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right)\\
								  &=  -\log\left(f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right)
								\end{align}
								$$</p>
								<h3>Summary of Cost Function</h3>
								<li>The cost function for logistic regression is: </li>
								<p>$$ J(\mathbf{w},b) = \frac{1}{m} \sum_{i=0}^{m-1} \left[ loss(f_{\mathbf{w},b}(\mathbf{x}^{(i)}), y^{(i)}) \right] \tag{1}$$</p>
								<p>where</p>
								<li><math xmlns="http://www.w3.org/1998/Math/MathML">
									<semantics>
									  <mrow>
										<mi>l</mi>
										<mi>o</mi>
										<mi>s</mi>
										<mi>s</mi>
										<mo stretchy="false">(</mo>
										<msub>
										  <mi>f</mi>
										  <mrow class="MJX-TeXAtom-ORD">
											<mrow class="MJX-TeXAtom-ORD">
											  <mi mathvariant="bold">w</mi>
											</mrow>
											<mo>,</mo>
											<mi>b</mi>
										  </mrow>
										</msub>
										<mo stretchy="false">(</mo>
										<msup>
										  <mrow class="MJX-TeXAtom-ORD">
											<mi mathvariant="bold">x</mi>
										  </mrow>
										  <mrow class="MJX-TeXAtom-ORD">
											<mo stretchy="false">(</mo>
											<mi>i</mi>
											<mo stretchy="false">)</mo>
										  </mrow>
										</msup>
										<mo stretchy="false">)</mo>
										<mo>,</mo>
										<msup>
										  <mi>y</mi>
										  <mrow class="MJX-TeXAtom-ORD">
											<mo stretchy="false">(</mo>
											<mi>i</mi>
											<mo stretchy="false">)</mo>
										  </mrow>
										</msup>
										<mo stretchy="false">)</mo>
									  </mrow>
									  <annotation encoding="application/x-tex">loss(f_{\mathbf{w},b}(\mathbf{x}^{(i)}), y^{(i)})</annotation>
									</semantics>
								  </math> is the cost for a single data point, which is:
								  </li>

								  <p>$$loss(f_{\mathbf{w},b}(\mathbf{x}^{(i)}), y^{(i)}) = -y^{(i)} \log\left(f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right) - \left( 1 - y^{(i)}\right) \log \left( 1 - f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right) \tag{2}$$</p>

								  <li>Where m is the number of training examples in the data set and:</li>
								  <p>$$
									\begin{align}
									  f_{\mathbf{w},b}(\mathbf{x^{(i)}}) &= g(z^{(i)})\tag{3} \\
									  z^{(i)} &= \mathbf{w} \cdot \mathbf{x}^{(i)}+ b\tag{4} \\
									  g(z^{(i)}) &= \frac{1}{1+e^{-z^{(i)}}}\tag{5} 
									\end{align}
									$$</p>
									<li><b>This is the Summary of Cost Function. For more about Cost function I would recommend you to take up Machine Learning Speciaization.</b></li>
					</section><!--//section-->
					
					<section class="docs-section" id="item-2-2">
						<h2 class="section-heading">Gradient Descent</h2>
						<p>Gradient descent (GD) is an iterative first-order optimisation algorithm used to find a local minimum/maximum of a given function. This method is commonly used in machine learning (ML) and deep learning(DL) to minimise a cost/loss function (e.g. in a linear regression).</p>
						<p>There are two specific requirements. A function has to be:</p>
						<li>Differentiable</li>
						<li>Convex</li>
						<p>There are other optimisation algorithms like Stochastic Gradient Descent which works on Non-Convex also. Other optimisation algorithms will be discussed in the future.</p>
						<p>$$\begin{align*}
							&\text{repeat until convergence:} \; \lbrace \\
							&  \; \; \;w_j = w_j -  \alpha \frac{\partial J(\mathbf{w},b)}{\partial w_j} \tag{1}  \; & \text{for j := 0..n-1} \\ 
							&  \; \; \;  \; \;b = b -  \alpha \frac{\partial J(\mathbf{w},b)}{\partial b} \\
							&\rbrace
							\end{align*}$$</p>

							<li>Where each iteration performs simultaneous updates on  ùë§<sub>ùëó</sub>  for all  ùëó , where</li>

						<p>$$\begin{align*}
							\frac{\partial J(\mathbf{w},b)}{\partial w_j}  &= \frac{1}{m} \sum\limits_{i = 0}^{m-1} (f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)} \tag{2} \\
							\frac{\partial J(\mathbf{w},b)}{\partial b}  &= \frac{1}{m} \sum\limits_{i = 0}^{m-1} (f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - y^{(i)}) \tag{3} 
							\end{align*}$$</p>

							<li>m is the number of training examples in the data set.</li>
							<li>ùëì<sub>ùê∞,ùëè</sub><sup>(ùë•<sup>(ùëñ)</sup>)</sup>  is the model's prediction, while ùë¶(ùëñ) is the target</li>
							<li>For a logistic regression model</li>
							<p>&emsp;&emsp;<math xmlns="http://www.w3.org/1998/Math/MathML">
								<semantics>
								  <mrow>
									<mi>z</mi>
									<mo>=</mo>
									<mrow class="MJX-TeXAtom-ORD">
									  <mi mathvariant="bold">w</mi>
									</mrow>
									<mo>&#x22C5;<!-- ‚ãÖ --></mo>
									<mrow class="MJX-TeXAtom-ORD">
									  <mi mathvariant="bold">x</mi>
									</mrow>
									<mo>+</mo>
									<mi>b</mi>
								  </mrow>
								  <annotation encoding="application/x-tex">z = \mathbf{w} \cdot \mathbf{x} + b</annotation>
								</semantics>
							  </math>
							  </p>
							  <p>&emsp;&emsp;<math xmlns="http://www.w3.org/1998/Math/MathML">
								<semantics>
								  <mrow>
									<msub>
									  <mi>f</mi>
									  <mrow class="MJX-TeXAtom-ORD">
										<mrow class="MJX-TeXAtom-ORD">
										  <mi mathvariant="bold">w</mi>
										</mrow>
										<mo>,</mo>
										<mi>b</mi>
									  </mrow>
									</msub>
									<mo stretchy="false">(</mo>
									<mi>x</mi>
									<mo stretchy="false">)</mo>
									<mo>=</mo>
									<mi>g</mi>
									<mo stretchy="false">(</mo>
									<mi>z</mi>
									<mo stretchy="false">)</mo>
								  </mrow>
								  <annotation encoding="application/x-tex">f_{\mathbf{w},b}(x) = g(z)</annotation>
								</semantics>
							  </math>
							  </p>
							  <p>
								&emsp;&emsp;where <b>ùëî(ùëß)</b>   is the sigmoid function:
							  </p>
							  <p>&emsp;&emsp;<math xmlns="http://www.w3.org/1998/Math/MathML">
								<semantics>
								  <mrow>
									<mi>g</mi>
									<mo stretchy="false">(</mo>
									<mi>z</mi>
									<mo stretchy="false">)</mo>
									<mo>=</mo>
									<mfrac>
									  <mn>1</mn>
									  <mrow>
										<mn>1</mn>
										<mo>+</mo>
										<msup>
										  <mi>e</mi>
										  <mrow class="MJX-TeXAtom-ORD">
											<mo>&#x2212;<!-- ‚àí --></mo>
											<mi>z</mi>
										  </mrow>
										</msup>
									  </mrow>
									</mfrac>
								  </mrow>
								  <annotation encoding="application/x-tex">g(z) = \frac{1}{1+e^{-z}}</annotation>
								</semantics>
							  </math>
							  </p>
					</section><!--//section-->
					
					
			    
			    
			    <article class="docs-article" id="section-3">
				    <header class="docs-header">
					    <h1 class="docs-heading">Overfitting & Underfitting</h1>
					    <section class="docs-intro">
						    <p>Generalization is the model‚Äôs ability to give sensible outputs to sets of input that it has never seen before.</p>
							<img class="figure-img img-fluid shadow rounded" src="/assets/images/Screenshot 2022-10-01 at 10.07.58 PM.png" alt="" title="Overfitting vs Underfitting">
							<br>
							<br><p>Overfitting and Underfitting refer to deficiencies that the model‚Äôs performance might suffer from. This means that knowing ‚Äúhow off‚Äù the model‚Äôs predictions are is a matter of knowing how close it is to overfitting or underfitting.</p>
							<li><b>A model that generalizes well is a model that is neither underfit nor overfit.</b></li>
						</section><!--//docs-intro-->
				    </header>

					<section class="docs-section" id="item-3-1">
						<h2 class="section-heading">Orginal Model</h2>
						<img class="figure-img img-fluid shadow rounded" src="https://miro.medium.com/max/1400/1*G_e9OocXWjmtLphidf2aLg.png" alt="" title="Orginal Model">
					</section><!--//section-->

				     <section class="docs-section" id="item-3-2">
						<h2 class="section-heading">Overfitting - (High Variance)</h2>
						<p>When we run our training algorithm on the data set, we allow the overall cost (i.e. distance from each point to the line) to become smaller with more iterations. Leaving this training algorithm run for long leads to minimal overall cost. However, this means that the line will be fit into all the points (including noise), catching secondary patterns that may not be needed for the generalizability of the model.</p>
						<li>If we leave the learning algorithm running for long time then:</li>
						<img class="figure-img img-fluid shadow rounded" src="https://miro.medium.com/max/1400/1*ffYTFTzj_00JDciD9KV19Q.png" alt="" title="Overfitting">
						<br>
						<br>
						<li><B>Overfitting is the case where the overall cost is really small, but the generalization of the model is unreliable. This is due to the model learning ‚Äútoo much‚Äù from the training data set.</B></li>
					</section><!--//section-->
					
					<section class="docs-section" id="item-3-3">
						<h2 class="section-heading">Underfitting - (High Bias)</h2>
						<p>We want the model to learn from the training data, but we don‚Äôt want it to learn too much (i.e. too many patterns). One solution could be to stop the training earlier. However, this could lead the model to not learn enough patterns from the training data, and possibly not even capture the dominant trend. This case is called underfitting.</p>
						<img class="figure-img img-fluid shadow rounded" src="https://miro.medium.com/max/1400/1*Gl5dciQc0H72vnZr5vIGqA.png" alt="" title="Underfitting">"
						<li><b>Underfitting is the case where the model has ‚Äú not learned enough‚Äù from the training data, resulting in low generalization and unreliable predictions.</b></li>
						<li><b>As you probably expected, underfitting (i.e. high bias) is just as bad for generalization of the model as overfitting. In high bias, the model might not have enough flexibility in terms of line fitting, resulting in a simplistic line that does not generalize well.</b></li>
					</section><!--//section-->
					
					<section class="docs-section" id="item-3-4">
						<h2 class="section-heading">Correct Fit - Bias-variance trade-off</h2>
						<p>So what is the right measure? Depending on the model at hand, a performance that lies between overfitting and underfitting is more desirable. This trade-off is the most integral aspect of Machine Learning model training. As we discussed, Machine Learning models fulfill their purpose when they generalize well.</p>
						<li>Generalization is bound by the two undesirable outcomes ‚Äî high bias and high variance. Detecting whether the model suffers from either one is the sole responsibility of the model developer.</li>
						<p>Check more about Overfitting and Underfitting from the <a href="https://towardsdatascience.com/what-are-overfitting-and-underfitting-in-machine-learning-a96b30864690"><b>LINK</b></a> and <a href="https://towardsdatascience.com/overfitting-and-underfitting-principles-ea8964d9c45c"><b>LINK</b></a>.</p>
					</section><!--//section-->
			    </article><!--//docs-article-->
			    
			    <article class="docs-article" id="section-4">
				    <header class="docs-header">
					    <h1 class="docs-heading">Regularization</h1>
					    <section class="docs-intro">
						    <p>Regularization is a technique used to reduce the errors by fitting the function appropriately on the given training set and avoid overfitting. </p>
							<p>The commonly used regularization techniques are :</p>
							<li>L1 Regularization</li>
							<li>L2 Regularization</li>
							<li>Dropout Regularization</li>
							<br>
							<p>L1 and L2 Regularization methods are:</p>
							<li>A regression model which uses L1 Regularization technique is called LASSO(Least Absolute Shrinkage and Selection Operator) regression. </li>
							<li>A regression model that uses L2 regularization technique is called Ridge regression. 
								Lasso Regression adds ‚Äúabsolute value of magnitude‚Äù of coefficient as penalty term to the loss function(L). </li>
						</section><!--//docs-intro-->
				    </header>
				     <section class="docs-section" id="item-4-1">
						<h2 class="section-heading">Regularization in Logistic and Linear </h2>
						<div class="row">
							
								<div class="column">
									<h4><center>Linear Gradient Regularization</center></h4>
									<img class="figure-img img-fluid shadow rounded" src="/assets/images/C1_W3_LinearGradientRegularized.png" alt="" title="Linear Gradient Regularization">
								</div>
								<div class="column">
									<h4><center>Logistic Gradient Regularization</center></h4>
									<img class="figure-img img-fluid shadow rounded" src="/assets/images/C1_W3_LogisticGradientRegularized.png" alt="" title="Logistic Gradient Regularization">
								</div>
						</div>
						<br>
								<p>The images above show the cost and gradient functions for both linear and logistic regression. Note:</p>
								<li>Cost - The cost functions differ significantly between linear and logistic regression, but adding regularization to the equations is the same.</li>
								<li>Gradient - The gradient functions for linear and logistic regression are very similar. They differ only in the implementation of  ùëì<sub>ùë§ùëè</sub>.</li>
					</section><!--//section-->
					
					<section class="docs-section" id="item-4-2">
						<h2 class="section-heading">Cost Function with Regularization</h2>
						<h3>-- Cost Function for Regularized Linear Regression</h3>
						<p>The equation for the cost function regularized linear regression is:</p>
						<p>$$J(\mathbf{w},b) = \frac{1}{2m} \sum\limits_{i = 0}^{m-1} (f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - y^{(i)})^2  + \frac{\lambda}{2m}  \sum_{j=0}^{n-1} w_j^2 \tag{1}$$ 
							where:
							$$ f_{\mathbf{w},b}(\mathbf{x}^{(i)}) = \mathbf{w} \cdot \mathbf{x}^{(i)} + b  \tag{2} $$ 
							</p>
							<li>Compare this to the cost function without regularization (Which is implemented above), which is of the form:</li>
							<p>$$J(\mathbf{w},b) = \frac{1}{2m} \sum\limits_{i = 0}^{m-1} (f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - y^{(i)})^2 $$ </p>
							<li>The difference is the regularization term:</li>
							<p>$$\frac{\lambda}{2m}  \sum_{j=0}^{n-1} w_j^2$$</p>
							<li>Including this term encourages gradient descent to minimize the size of the parameters. Note, in this example, the parameter  ùëè  is not regularized. This is standard practice.</li>
						<br>
						<h3>-- Cost Function for Regularized Logistic Regression</h3>
						<li>For regularized logistic regression, the cost function is of the form:</li>
						<p>$$J(\mathbf{w},b) = \frac{1}{m}  \sum_{i=0}^{m-1} \left[ -y^{(i)} \log\left(f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right) - \left( 1 - y^{(i)}\right) \log \left( 1 - f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right) \right] + \frac{\lambda}{2m}  \sum_{j=0}^{n-1} w_j^2 \tag{3}$$</p>
						<p>where:</p>
						<p>$$ f_{\mathbf{w},b}(\mathbf{x}^{(i)}) = sigmoid(\mathbf{w} \cdot \mathbf{x}^{(i)} + b)  \tag{4} $$ </p>
						<li>Compare this to the cost function without regularization (which is implemented above):</li>
						<p>$$ J(\mathbf{w},b) = \frac{1}{m}\sum_{i=0}^{m-1} \left[ (-y^{(i)} \log\left(f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right) - \left( 1 - y^{(i)}\right) \log \left( 1 - f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right)\right] $$</p>
						<li>As was the case in linear regression above, the difference is the regularization term, which is:</li>
						<p>$$\frac{\lambda}{2m}  \sum_{j=0}^{n-1} w_j^2$$</p>
						<li>Including this term encourages gradient descent to minimize the size of the parameters. Note, in this example, the parameter  ùëè  is not regularized. This is standard practice.</li>
					</section><!--//section-->
					
					<section class="docs-section" id="item-4-3">
						<h2 class="section-heading">Gradient Descent with Regularization</h2>
						<li>The basic algorithm for running gradient descent does not change with regularization, it is:</li>
						<p>$$\begin{align*}
							&\text{repeat until convergence:} \; \lbrace \\
							&  \; \; \;w_j = w_j -  \alpha \frac{\partial J(\mathbf{w},b)}{\partial w_j} \tag{1}  \; & \text{for j := 0..n-1} \\ 
							&  \; \; \;  \; \;b = b -  \alpha \frac{\partial J(\mathbf{w},b)}{\partial b} \\
							&\rbrace
							\end{align*}$$</p>

						<li>Where each iteration performs simultaneous updates on  ùë§<sub>ùëó</sub>  for all  ùëó.</li>

						<h3>-- Computing the Gradient with regularization (both linear/logistic)</h3>
						<li>The gradient calculation for both linear and logistic regression are nearly identical, differing only in computation of  ùëì<sub>ùê∞ùëè</sub>.</li>
						<p>$$\begin{align*}
							\frac{\partial J(\mathbf{w},b)}{\partial w_j}  &= \frac{1}{m} \sum\limits_{i = 0}^{m-1} (f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)}  +  \frac{\lambda}{m} w_j \tag{2} \\
							\frac{\partial J(\mathbf{w},b)}{\partial b}  &= \frac{1}{m} \sum\limits_{i = 0}^{m-1} (f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - y^{(i)}) \tag{3} 
							\end{align*}$$</p>
						<li>m is the number of training examples in the data set</li>
						<li><math xmlns="http://www.w3.org/1998/Math/MathML">
							<semantics>
							  <mrow>
								<msub>
								  <mi>f</mi>
								  <mrow class="MJX-TeXAtom-ORD">
									<mrow class="MJX-TeXAtom-ORD">
									  <mi mathvariant="bold">w</mi>
									</mrow>
									<mo>,</mo>
									<mi>b</mi>
								  </mrow>
								</msub>
								<mo stretchy="false">(</mo>
								<msup>
								  <mi>x</mi>
								  <mrow class="MJX-TeXAtom-ORD">
									<mo stretchy="false">(</mo>
									<mi>i</mi>
									<mo stretchy="false">)</mo>
								  </mrow>
								</msup>
								<mo stretchy="false">)</mo>
							  </mrow>
							  <annotation encoding="application/x-tex">f_{\mathbf{w},b}(x^{(i)})</annotation>
							</semantics>
						  </math> is the model's prediction, while <math xmlns="http://www.w3.org/1998/Math/MathML">
							<semantics>
							  <msup>
								<mi>y</mi>
								<mrow class="MJX-TeXAtom-ORD">
								  <mo stretchy="false">(</mo>
								  <mi>i</mi>
								  <mo stretchy="false">)</mo>
								</mrow>
							  </msup>
							  <annotation encoding="application/x-tex">y^{(i)}</annotation>
							</semantics>
						  </math> is the target.						  
						  </li>
						  <li>For a linear regression model:</li>
						  <p>$$f_{\mathbf{w},b}(x) = \mathbf{w} \cdot \mathbf{x} + b$$</p>
						  <li>For a logistic regression model:</li>
						  <p>$$z = \mathbf{w} \cdot \mathbf{x} + b$$</p>
						  <p>$$f_{\mathbf{w},b}(x) = g(z)$$</p>
						  <li>where  ùëî(ùëß)  is the sigmoid function:</li>
						  <p>$$g(z) = \frac{1}{1+e^{-z}}$$</p>
						  <li>The term which adds regularization is the <math xmlns="http://www.w3.org/1998/Math/MathML">
							<semantics>
							  <mrow>
								<mfrac>
								  <mi>&#x03BB;<!-- Œª --></mi>
								  <mi>m</mi>
								</mfrac>
								<msub>
								  <mi>w</mi>
								  <mi>j</mi>
								</msub>
							  </mrow>
							  <annotation encoding="application/x-tex">\frac{\lambda}{m} w_j</annotation>
							</semantics>
						  </math>.
						  </li>
					</section><!--//section-->
			    </article><!--//docs-article-->
			    
			    <article class="docs-article" id="section-5">
				    <header class="docs-header">
					    <h1 class="docs-heading">Logistic Regression using Scikit-Learn</h1>
					    <section class="docs-intro">
						    <p>Here we will view how Logistic Regression is implemented using Scikit Learn.</p>
						</section><!--//docs-intro-->
				    </header>
				     <section class="docs-section" id="item-5-1">
						<h2 class="section-heading">Model</h2>
						<p><a href="https://scikit-learn.org/">Scikit-Learn</a> is a machine learning library most commonly used by machine learning researchers, students and many more.</p>
						<li>Click this Link ---> <a href="#">Logistic Regression</a> using Scikit-Learn to know in depth about Logistic Regression.</li>

					</section><!--//section-->
			    </article><!--//docs-article-->
			    
			    
		        <article class="docs-article" id="section-6">
				    <header class="docs-header">
					    <h1 class="docs-heading">Conclusion</h1>
					    <section class="docs-intro">
						    <p>This is the end of Course 1 of Machine Learning Specialization. Let's got to Course 2 of this specialization in upcoming days.</p>
						</section><!--//docs-intro-->
				    </header>
				     <section class="docs-section" id="item-6-1">
						<h2 class="section-heading">What we have covered?</h2>
						<li>Learn about Logistic Regression</li>
						<li>Implement Logistic Regression for Binary Classification</li>
						<li>Learn about Overfitting, Underfitting and Generalization</li>
						<li>Learn about Regularization for Linear and Logistic Regression</li>
					</section><!--//section-->
			    </article><!--//docs-article-->
			    
			    
			    

			    <footer class="footer">
				    <div class="container text-center py-5">
				        <!--/* This template is free as long as you keep the footer attribution link. If you'd like to use the template without the attribution link, you can buy the commercial license via our website: themes.3rdwavemedia.com Thank you for your support. :) */-->
			            <!-- <small class="copyright">Designed with <span class="sr-only">love</span><i class="fas fa-heart" style="color: #fb866a;"></i> by <a class="theme-link" href="http://themes.3rdwavemedia.com" target="_blank">Xiaoying Riley</a> for developers</small> -->
				        <ul class="social-list list-unstyled pt-4 mb-0">
						    <li class="list-inline-item"><a href="https://github.com/bhanuprasanna527" target="_blank" rel="noopener noreferrer"><i class="fab fa-github fa-fw"></i></a></li>
			            <li class="list-inline-item"><a href="https://twitter.com/bhanu527" target="_blank" rel="noopener noreferrer"><i class="fab fa-twitter fa-fw"></i></a></li>
		                <li class="list-inline-item"><a href="https://www.linkedin.com/in/bhanu-prasanna-a1b71b1a2/" target="_blank" rel="noopener noreferrer"><i class="fab fa-linkedin fa-fw"></i></a></li>
		                <li class="list-inline-item"><a href="https://www.instagram.com/bhanuprasanna527/" target="_blank" rel="noopener noreferrer"><i class="fab fa-instagram fa-fw"></i></a></li>
				        </ul><!--//social-list-->
				    </div>
			    </footer>
		    </div> 
	    </div>
    </div><!--//docs-wrapper-->
   
       
    <!-- Javascript -->          
    <script src="/assets/plugins/popper.min.js"></script>
    <script src="/assets/plugins/bootstrap/js/bootstrap.min.js"></script>  
    
    
    <!-- Page Specific JS -->
    <script src="/assets/plugins/smoothscroll.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.8/highlight.min.js"></script>
    <script src="/assets/js/highlight-custom.js"></script> 
    <script src="/assets/plugins/simplelightbox/simple-lightbox.min.js"></script>      
    <script src="/assets/plugins/gumshoe/gumshoe.polyfills.min.js"></script> 
    <script src="/assets/js/docs.js"></script> 

</body>
</html> 

